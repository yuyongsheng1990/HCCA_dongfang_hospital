{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2022.3.9\\n术后预后模型：\\n    生存时间模型\\n    风险评分模型\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2022.3.9\n",
    "术后预后模型：\n",
    "    生存时间模型\n",
    "    风险评分模型\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model =pd.read_excel(project_path +'/data/result/feature_engineering/df_4.5_model_data_forward_after_accuracy.xlsx')\n",
    "if 'Unnamed: 0' in df_model.columns:\n",
    "    df_model = df_model.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'biliary_disease', 'AJCC_8', 'MSKCC', 'gender', 'smoke',\n",
       "       'drinking', 'blood_type', 'HBcAb', 'DB', 'emaciation', 'tumor_AFP',\n",
       "       'tumor_CEA', 'tumor_CA199', 'tumor_CA125', 'tumor_size', 'HBsAg', 'LC',\n",
       "       'PTCD_ERCP', 'surgery_bleeding', 'surgery_plasm', 'surgery_result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(417, 22)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    129\n",
       "1    111\n",
       "0    100\n",
       "3     77\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_col=['biliary_disease','AJCC_8', 'MSKCC', 'gender', 'smoke','drinking', 'blood_type', 'HBcAb',\n",
    "             'emaciation','HBsAg', 'LC','PTCD_ERCP','surgery_result']\n",
    "\n",
    "continuous_col=[x for x in df_model.columns if x not in discrete_col]\n",
    "continuous_col.remove('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 防止不同维特征数据差距过大，影响建模效果\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "# df_model[continuous_col]=ss.fit_transform(df_model[continuous_col])\n",
    "for i in continuous_col:\n",
    "    df_model[[i]] = ss.fit_transform(df_model[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'biliary_disease', 'AJCC_8', 'MSKCC', 'gender', 'smoke',\n",
       "       'drinking', 'blood_type', 'HBcAb', 'DB', 'emaciation', 'tumor_AFP',\n",
       "       'tumor_CEA', 'tumor_CA199', 'tumor_CA125', 'tumor_size', 'HBsAg', 'LC',\n",
       "       'PTCD_ERCP', 'surgery_bleeding', 'surgery_plasm', 'surgery_result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.to_excel(project_path+'/data/result/modeling/df_1.2_数据归一化.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 插补数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机森林对缺失值进行插补\n",
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def missing_value_interpolation(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    # 提取存在缺失值的列名\n",
    "    missing_list = []\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().sum()>0:\n",
    "            missing_list.append(i)\n",
    "    missing_list_copy = missing_list.copy()\n",
    "    # 用该列未缺失的值训练随机森林，然后用训练好的rf预测缺失值\n",
    "    for i in range(len(missing_list)):\n",
    "        name=missing_list[0]\n",
    "        df_missing = df[missing_list_copy]\n",
    "        # 将其他列的缺失值用0表示。\n",
    "        missing_list.remove(name)\n",
    "        for j in missing_list:\n",
    "            df_missing[j]=df_missing[j].astype('str').apply(lambda x: 0 if x=='nan' else x)\n",
    "        df_missing_is = df_missing[df_missing[name].isnull()]\n",
    "        df_missing_not = df_missing[df_missing[name].notnull()]\n",
    "        y = df_missing_not[name]\n",
    "        x = df_missing_not.drop([name],axis=1)\n",
    "\n",
    "        rfr = RandomForestRegressor(n_estimators=300,\n",
    "                                    random_state=3)\n",
    "        rfr.fit(x, y)\n",
    "        #预测缺失值\n",
    "        predict = rfr.predict(df_missing_is.drop([name],axis=1))\n",
    "        #填补缺失值\n",
    "        df.loc[df[name].isnull(),name] = predict\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5726177404d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 插补建模数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_model_cb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmissing_value_interpolation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# df_model_cb=df_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-09d3f8318f77>\u001b[0m in \u001b[0;36mmissing_value_interpolation\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     28\u001b[0m         rfr = RandomForestRegressor(n_estimators=300,\n\u001b[0;32m     29\u001b[0m                                     random_state=3)\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mrfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;31m#预测缺失值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_missing_is\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    391\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[1;32m--> 393\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    167\u001b[0m                                                         indices=indices)\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1257\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    392\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 插补建模数据\n",
    "df_model_cb=missing_value_interpolation(df_model)\n",
    "# df_model_cb=df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_cb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存插补数据\n",
    "df_model_cb.to_excel(project_path + '/data/result/modeling/df_1.3_model_data_插补.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score,average_precision_score,precision_recall_curve\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score,accuracy_score\n",
    "\n",
    "# 划分训练集和测试集，比例为8:2\n",
    "x = df_model_cb.drop(['label'],axis=1)\n",
    "y = df_model_cb['label']\n",
    "\n",
    "seeds_list=[]\n",
    "cat_f1_list=[]\n",
    "for i in range(101):\n",
    "    \n",
    "    tran_x, test_x, tran_y, test_y = train_test_split(x, y, test_size=0.2, random_state=i)\n",
    "    \n",
    "    # 分类数据过采样\n",
    "    from imblearn.over_sampling import SMOTE,ADASYN \n",
    "    sm = SMOTE(random_state=0)\n",
    "    tran_x_sm,tran_y_sm = sm.fit_resample(tran_x,tran_y)\n",
    "#     tran_x_sm,tran_y_sm=tran_x,tran_y\n",
    "    \n",
    "    import xgboost\n",
    "    cat_model=xgboost.XGBClassifier(max_depth=5,\n",
    "                        learning_rate=0.01,\n",
    "                        n_estimators=500,\n",
    "                        min_child_weight=0.5,\n",
    "                        eta=0.1,\n",
    "                        gamma=0.5,\n",
    "                        reg_lambda=10,\n",
    "                        subsample=0.5,\n",
    "                        colsample_bytree=0.8,\n",
    "                        nthread=4,\n",
    "                        scale_pos_weight=1,\n",
    "                        random_state=3)\n",
    "    # 分类模型\n",
    "    cat_model.fit(tran_x_sm,tran_y_sm)\n",
    "    cat_predictions=cat_model.predict(test_x)\n",
    "    cat_f1=f1_score(test_y,cat_predictions,average='macro')\n",
    "    # 防止分类数据的测试集划分不平衡\n",
    "    if not (2 >=(test_y.value_counts().values[0])/(test_y.value_counts().values[-1]) >=1):\n",
    "        continue\n",
    "\n",
    "#     import catboost\n",
    "#     # CatBoost模型\n",
    "#     cat_model=catboost.CatBoostRegressor(iterations=300, \n",
    "#                                           learning_rate=0.2, \n",
    "#                                           depth=6,\n",
    "#                                           l2_leaf_reg=2,\n",
    "#                                           subsample=1,\n",
    "#                                           loss_function='RMSE', # 'CrossEntropy',\n",
    "#                                           random_state=3)\n",
    "#     # 回归模型\n",
    "#     cat_model.fit(tran_x,tran_y)\n",
    "#     cat_predictions=cat_model.predict(test_x)\n",
    "#     cat_f1=r2_score(test_y,cat_predictions)\n",
    "    \n",
    "    seeds_list.append(i)\n",
    "    cat_f1_list.append(cat_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "df_seeds=pd.DataFrame(data={'seed':seeds_list,\n",
    "                           'cat_f1':cat_f1_list})\n",
    "df_seeds=df_seeds.sort_values(['cat_f1'], ascending=0).reset_index(drop=True)\n",
    "df_seeds.to_excel(project_path+'/data/df_seeds.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_seeds.loc[0,'seed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类随机数种子\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 划分训练集和测试集，比例为8:2\n",
    "x = df_model_cb.drop(['label'],axis=1)\n",
    "y = df_model_cb['label']\n",
    "\n",
    "seed_index=df_seeds.loc[0,'seed']\n",
    "tran_x, test_x, tran_y, test_y = train_test_split(x, y, test_size=0.2, random_state=seed_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tran_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行过采样\n",
    "from imblearn.over_sampling import SMOTE,ADASYN \n",
    "from imblearn.combine import SMOTETomek\n",
    "sm = SMOTE(random_state=0)\n",
    "# sm=ADASYN(random_state=0)\n",
    "\n",
    "tran_x_sm,tran_y_sm = sm.fit_resample(tran_x,tran_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_x_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_y_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_x_sm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_x_sm.to_excel(project_path+'/data/tran_x_sm.xlsx')\n",
    "tran_y_sm.to_excel(project_path+'/data/tran_y_sm.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类模型：5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold \n",
    "\n",
    "# 划分训练集和测试集，比例为8:2\n",
    "x = df_model_cb.drop(['label'],axis=1)\n",
    "y = df_model_cb['label']\n",
    "# 五折交叉验证\n",
    "kf = KFold(n_splits=5,random_state=seed_index,shuffle=True)\n",
    "\n",
    "df_auc_accuracy=pd.DataFrame()\n",
    "df_precision_f1=pd.DataFrame()\n",
    "temp_importance_list=[]\n",
    "\n",
    "for train_index, test_index in kf.split(x):\n",
    "\n",
    "    tran_x,test_x,tran_y,test_y=x.values[train_index],x.values[test_index],y.values[train_index],y.values[test_index]\n",
    "\n",
    "    tran_x_sm,tran_y_sm = sm.fit_resample(tran_x,tran_y)\n",
    "    \n",
    "    from sklearn.metrics import r2_score,average_precision_score,precision_recall_curve,brier_score_loss\n",
    "    from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score,accuracy_score\n",
    "    import xgboost\n",
    "    # XGBoost模型\n",
    "    xgb_model=xgboost.XGBClassifier(max_depth=3,\n",
    "                            learning_rate=0.01,\n",
    "                            n_estimators=500,\n",
    "                            min_child_weight=0.3,\n",
    "                            eta=0.1,\n",
    "                            gamma=0.4,\n",
    "                            reg_lambda=10,\n",
    "                            subsample=0.8,\n",
    "                            colsample_bytree=0.8,\n",
    "                            nthread=4,\n",
    "                            scale_pos_weight=1,\n",
    "                            random_state=3)\n",
    "    xgb_model.fit(tran_x_sm,tran_y_sm)\n",
    "    xgb_predictions=xgb_model.predict(test_x)\n",
    "\n",
    "\n",
    "    import lightgbm\n",
    "    # LightGBM模型\n",
    "    lgbm_model=lightgbm.LGBMClassifier(iterations=300, \n",
    "                                      max_depth=4,\n",
    "                                      min_child_weight=0.5,\n",
    "                                      gamma=0.5,\n",
    "                                       reg_lambda=5,\n",
    "                                      subsample=0.8,\n",
    "                                      learning_rate=0.02, \n",
    "                                      loss_function='CrossEntropy',\n",
    "                                      random_state=3)\n",
    "    lgbm_model.fit(tran_x_sm,tran_y_sm)\n",
    "    lgbm_predictions=lgbm_model.predict(test_x)\n",
    "\n",
    "\n",
    "    import catboost\n",
    "    # CatBoost模型\n",
    "    cat_model=catboost.CatBoostClassifier(iterations=400, \n",
    "                                          learning_rate=0.09, \n",
    "                                          depth=3,\n",
    "                                          l2_leaf_reg=2,\n",
    "                                          loss_function='MultiClass',\n",
    "                                          random_state=3)\n",
    "    cat_model.fit(tran_x_sm,tran_y_sm)\n",
    "    cat_predictions=cat_model.predict(test_x)\n",
    "    \n",
    "    # 随机森林\n",
    "    from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    # 列出参数列表\n",
    "    tree_grid_parameter = {'n_estimators': list((10, 50, 100, 150, 200))}\n",
    "    # 进行参数的搜索组合\n",
    "    grid = GridSearchCV(RandomForestClassifier(), param_grid=tree_grid_parameter, cv=3)\n",
    "    # 根据已有数据去拟合随机森林模型\n",
    "    grid.fit(tran_x_sm, tran_y_sm)\n",
    "    rf_model = RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'],\n",
    "                                max_depth=8,\n",
    "                                random_state=3)\n",
    "    rf_model.fit(tran_x_sm, tran_y_sm)\n",
    "    # 预测缺失值\n",
    "    rf_predictions = rf_model.predict(test_x)\n",
    "\n",
    "\n",
    "    # GBDT\n",
    "    # 列出参数列表\n",
    "    gbdt_model = GradientBoostingClassifier(n_estimators=300,\n",
    "                                learning_rate=0.1,\n",
    "                                max_depth=3,\n",
    "                                subsample=0.5,\n",
    "                                random_state=3)\n",
    "    gbdt_model.fit(tran_x_sm,tran_y_sm)\n",
    "    # 预测缺失值\n",
    "    gbdt_predictions = gbdt_model.predict(test_x)\n",
    "\n",
    "\n",
    "    # SVR\n",
    "    from sklearn.svm import SVR,SVC\n",
    "    # 回归模型\n",
    "    # svr = SVR(kernel='linear', C=1.25)\n",
    "    # 分类模型\n",
    "    svr_model = SVC(kernel='rbf',\n",
    "                    C=50,\n",
    "                    cache_size=200,\n",
    "                    probability=True,\n",
    "                    random_state=3)\n",
    "    svr_model.fit(tran_x_sm,tran_y_sm)\n",
    "    svr_predictions=svr_model.predict(test_x)\n",
    "\n",
    "\n",
    "    # Linear回归，Lasso回归，领回归，logistic回归\n",
    "    from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet,LogisticRegression\n",
    "    lcv_model = LogisticRegression(penalty='l2',\n",
    "                             C=5,\n",
    "                            solver='lbfgs',\n",
    "                             max_iter=100,\n",
    "                            random_state=3)\n",
    "    # lcv = Lasso()\n",
    "    # lcv = Ridge()\n",
    "    lcv_model.fit(tran_x_sm, tran_y_sm)\n",
    "    lcv_predictions = lcv_model.predict(test_x)\n",
    "\n",
    "    # ANN\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "    ANN_model = MLPClassifier(alpha=0.1, \n",
    "                        hidden_layer_sizes=[100,], \n",
    "                        solver='adam', \n",
    "                        activation='relu', \n",
    "                        random_state=3)\n",
    "    ANN_model.fit(tran_x_sm, tran_y_sm)\n",
    "    ANN_predictions=ANN_model.predict(test_x)\n",
    "\n",
    "    # TabNet\n",
    "    import torch\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "    from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "    TabNet_model = TabNetMultiTaskClassifier(\n",
    "                            n_d=8,\n",
    "                            n_a=8,\n",
    "                            n_steps=5,\n",
    "                            gamma=1,\n",
    "#                            optimizer_fn=torch.optim.Adam,\n",
    "#                            optimizer_params=dict(lr=2e-2),\n",
    "#                            scheduler_params={\"step_size\":50, # how to use learning rate scheduler\n",
    "#                                              \"gamma\":0.9},\n",
    "#                            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                           mask_type='entmax') # \"sparsemax\"\n",
    "    tran_x_x, tran_x_valid, tran_y_y, tran_y_valid = train_test_split(tran_x_sm, tran_y_sm, test_size=0.125, random_state=3)\n",
    "\n",
    "    TabNet_model.fit(X_train=tran_x_sm, \n",
    "            y_train=tran_y_sm.reshape(-1,1),\n",
    "            max_epochs=200, \n",
    "            patience=50,\n",
    "            batch_size=64, \n",
    "            virtual_batch_size=16,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            loss_fn=[torch.nn.functional.cross_entropy]) # Optional, just an example of list usage\n",
    "    TabNet_predictions=TabNet_model.predict(test_x)\n",
    "\n",
    "    # 计算评价指标compute evaluation metrics\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "    # 统一模型输出结果\n",
    "    df_model_result=pd.DataFrame(\n",
    "        columns=['model','index','precision','recall','f1-score','support','accuracy','AUC','sensitivity','specificity'])\n",
    "\n",
    "    model_list=[xgb_model,lgbm_model,cat_model,rf_model,gbdt_model,svr_model,lcv_model,ANN_model,TabNet_model]\n",
    "    model_name_list=['XGBoost','LGBM','CatBoost','RF','GBDT','SVR','LR','ANN','TabNet']\n",
    "#     model_list=[TabNet_model]\n",
    "#     model_name_list=['TabNet']\n",
    "    \n",
    "    temp_auc=pd.DataFrame()    \n",
    "    for model,name in zip(model_list,model_name_list):\n",
    "        print(name)\n",
    "        # 计算accuracy和AUC\n",
    "        # tabnet predict_proba结果是三维数组，无法计算auc，需要reshape(-1,6),所有行 x 6列\n",
    "        test_y_score=np.reshape(model.predict_proba(test_x),(-1,4))\n",
    "        auc=roc_auc_score(test_y,test_y_score,multi_class='ovr')\n",
    "        auc=round(auc,2)\n",
    "        # tabnet predict结果是三维数组，无法计算auc，需要reshape\n",
    "        predictions=np.reshape(model.predict(test_x),(-1,1)).astype(str)\n",
    "        accuracy=accuracy_score(test_y.astype(str),predictions)\n",
    "        accuracy=round(accuracy,2)\n",
    "        # 计算precision、recall、F1\n",
    "        precision=precision_score(test_y.astype(str),predictions,average='macro')\n",
    "        precision=round(precision,2)\n",
    "        recall=recall_score(test_y.astype(str),predictions,average='macro')\n",
    "        recall=round(recall,2)\n",
    "        f1=f1_score(test_y.astype(str),predictions,average='macro')\n",
    "        f1=round(f1,2)\n",
    "        temp_auc.loc[temp_auc.shape[0],['model','precision','recall','f1','accuracy','AUC']]=\\\n",
    "                                                                    [name,precision,recall,f1,accuracy,auc]\n",
    "    df_auc_accuracy=pd.concat([df_auc_accuracy,temp_auc],axis=0)\n",
    "                                       \n",
    "    # 变量重要性评分\n",
    "    importance = xgb_model.feature_importances_\n",
    "    df_importance_temp=pd.DataFrame(data={'特征':x.columns,'重要性评分':importance})\n",
    "    temp_importance_list.append(df_importance_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result=df_auc_accuracy.groupby(['model'])[['precision','recall','f1','accuracy','AUC']].mean().reset_index()\n",
    "df_model_result=df_model_result.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 排序\n",
    "df_model_result['temp_num']=[8,3,5,2,6,4,7,9,1]\n",
    "df_model_result=df_model_result.sort_values(by=['temp_num'],ascending=True).drop(['temp_num'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型测试效果\n",
    "df_model_result.to_excel(project_path+'/data/result/modeling/df_1.6_分类_模型测试效果_5折交叉验证_after.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance=pd.concat(temp_importance_list,axis=0)\n",
    "df_importance=df_importance.groupby(['特征'])[['重要性评分']].mean().sort_values(['重要性评分'],ascending=False).reset_index()\n",
    "df_importance=df_importance.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance.to_excel(project_path+'/data/result/modeling/df_1.6_模型重要性评分_after.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 术后风险评分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建术后生存分析数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance =pd.read_excel(project_path +'/data/result/modeling/df_1.6_模型重要性评分_after.xlsx')\n",
    "if 'Unnamed: 0' in df_importance.columns:\n",
    "    df_importance = df_importance.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_col=df_importance['特征'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surgery_result',\n",
       " 'AJCC_8',\n",
       " 'blood_type',\n",
       " 'tumor_CA19-9',\n",
       " 'MSKCC',\n",
       " 'tumor_CA125',\n",
       " 'HBcAb',\n",
       " 'surgery_plasm',\n",
       " 'HBsAg',\n",
       " 'surgery_bleeding',\n",
       " 'tumor_size',\n",
       " 'smoke',\n",
       " 'emaciation',\n",
       " 'drinking',\n",
       " 'DB',\n",
       " 'tumor_AFP',\n",
       " 'tumor_CEA',\n",
       " 'PTCD_ERCP',\n",
       " 'biliary_disease',\n",
       " 'gender',\n",
       " 'LC']"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(importance_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入生存期和标签\n",
    "survival_col=['target_3Y_death','target_survival_month']\n",
    "importance_col.extend(survival_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surgery_result',\n",
       " 'AJCC_8',\n",
       " 'blood_type',\n",
       " 'tumor_CA19-9',\n",
       " 'MSKCC',\n",
       " 'tumor_CA125',\n",
       " 'HBcAb',\n",
       " 'surgery_plasm',\n",
       " 'HBsAg',\n",
       " 'surgery_bleeding',\n",
       " 'tumor_size',\n",
       " 'smoke',\n",
       " 'emaciation',\n",
       " 'drinking',\n",
       " 'DB',\n",
       " 'tumor_AFP',\n",
       " 'tumor_CEA',\n",
       " 'PTCD_ERCP',\n",
       " 'biliary_disease',\n",
       " 'gender',\n",
       " 'LC',\n",
       " 'target_3Y_death',\n",
       " 'target_survival_month']"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_cox_after =pd.read_excel(project_path +'/data/processed_data/df_3.1.1_术后预后cox数据集.xlsx')\n",
    "if 'Unnamed: 0' in df_dataset_cox_after.columns:\n",
    "    df_dataset_cox_after = df_dataset_cox_after.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451, 75)"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_cox_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_cox_after_importance=df_dataset_cox_after[importance_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451, 23)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_cox_after_importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['surgery_result', 'AJCC_8', 'blood_type', 'tumor_CA19-9', 'MSKCC',\n",
       "       'tumor_CA125', 'HBcAb', 'surgery_plasm', 'HBsAg', 'surgery_bleeding',\n",
       "       'tumor_size', 'smoke', 'emaciation', 'drinking', 'DB', 'tumor_AFP',\n",
       "       'tumor_CEA', 'PTCD_ERCP', 'biliary_disease', 'gender', 'LC',\n",
       "       'target_3Y_death', 'target_survival_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_cox_after_importance.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_col=['surgery_result', 'AJCC_8', 'blood_type','MSKCC','HBcAb', 'surgery_plasm', 'HBsAg', 'surgery_bleeding','smoke', \n",
    "              'emaciation', 'drinking','PTCD_ERCP', 'biliary_disease', 'gender', 'LC', 'target_3Y_death']\n",
    "continuous_col=[x for x in df_dataset_cox_after_importance.columns if x not in discrete_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 筛选随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest,shapiro\n",
    "##检验是否正态\n",
    "def norm_test(data):\n",
    "    if len(data) > 30:\n",
    "        norm, p = kstest(data, 'norm')\n",
    "    else:\n",
    "        norm, p = shapiro(data)\n",
    "    #print(t,p)\n",
    "    if p>=0.05:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "# 连续变量的显著性检验\n",
    "def test2(data_b, data_p):\n",
    "    if norm_test(data_b) and norm_test(data_p):\n",
    "        x = 1\n",
    "        y = '独立样本T检验'\n",
    "        t, p = st.ttest_ind(list(data_b),list(data_p), nan_policy='omit')\n",
    "    else:\n",
    "        x = 0\n",
    "        y = 'Mann-Whitney U检验'\n",
    "        t,p = st.mannwhitneyu(list(data_b),list(data_p))\n",
    "    return x,y,t,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_test(df_high,df_low,list1):\n",
    "\n",
    "    feature_list=[]  # 特征列表\n",
    "    y_list=[]  # 显著性检验方法\n",
    "    t_list=[]  # 统计量\n",
    "    p_list=[]  # p值\n",
    "    result_list=[]  # 是否显著\n",
    "    high_mean_list=[]\n",
    "    low_mean_list=[]\n",
    "\n",
    "    for i in list1:\n",
    "        print(i)\n",
    "        # 高剂量组统计\n",
    "        df_high_nt=df_high[df_high[i].notnull()]\n",
    "        data_high=df_high_nt[i]\n",
    "        high_mean=round(data_high.mean(),2)\n",
    "        \n",
    "        df_low_nt=df_low[df_low[i].notnull()]\n",
    "        data_low=df_low_nt[i]\n",
    "        low_mean=round(data_low.mean(),2)\n",
    "\n",
    "        # 计算高低剂量组显著性差异\n",
    "        if data_high.shape[0] >= 10 and data_low.shape[0]>=10:\n",
    "            # 连续变量检验\n",
    "            x,y,t,p = test2(data_high, data_low)\n",
    "            t=round(t,2)\n",
    "            p=round(p,3)\n",
    "            if p <=0.05:\n",
    "                sig='显著'\n",
    "            else:\n",
    "                sig='不显著'\n",
    "            # 显著性 \n",
    "            feature_list.append(i)\n",
    "            y_list.append(y)\n",
    "            t_list.append(t)\n",
    "            p_list.append(p)\n",
    "            result_list.append(sig)\n",
    "            high_mean_list.append(high_mean)\n",
    "            low_mean_list.append(low_mean)\n",
    "\n",
    "    df_result=pd.DataFrame({'特征':feature_list,\n",
    "                            '高剂量均值':high_mean_list,\n",
    "                            '低剂量均值':low_mean_list,\n",
    "                            '检验指标':y_list,\n",
    "                            't值':t_list,\n",
    "                            'p值':p_list,\n",
    "                            '显著性结果':result_list})\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n",
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n"
     ]
    }
   ],
   "source": [
    "# 筛选出划分效果最均衡的随机数种子\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 划分训练集和测试集，比例为8:2\n",
    "seed_index_list=[]\n",
    "p_mean_list=[]\n",
    "for i in range(100):\n",
    "    df_after_tran, df_after_test = train_test_split(df_dataset_cox_after_importance, test_size=0.2, random_state=i)\n",
    "    df_continuous_sig = sig_test(df_after_tran,df_after_test,continuous_col)\n",
    "    p_mean=df_continuous_sig['p值'].mean() + df_continuous_sig['p值'].std()\n",
    "    seed_index_list.append(i)\n",
    "    p_mean_list.append(p_mean)\n",
    "    \n",
    "df_p_mean=pd.DataFrame(data={'seed_index':seed_index_list,\n",
    "                            'p_mean':p_mean_list})\n",
    "df_p_mean=df_p_mean.sort_values(['p_mean'],ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_index</th>\n",
       "      <th>p_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0.508878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>0.502905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0.489131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>0.487529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0.486739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>94</td>\n",
       "      <td>0.274103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>37</td>\n",
       "      <td>0.271496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>0.247287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>17</td>\n",
       "      <td>0.235961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10</td>\n",
       "      <td>0.175361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed_index    p_mean\n",
       "0           75  0.508878\n",
       "1           96  0.502905\n",
       "2           41  0.489131\n",
       "3           42  0.487529\n",
       "4           25  0.486739\n",
       "..         ...       ...\n",
       "95          94  0.274103\n",
       "96          37  0.271496\n",
       "97           3  0.247287\n",
       "98          17  0.235961\n",
       "99          10  0.175361\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_index=df_p_mean.loc[0,'seed_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 划分训练集和测试集，比例为8:2\n",
    "df_after_ML_tran, df_after_ML_test = train_test_split(df_dataset_cox_after_importance, test_size=0.2,\n",
    "                                                                random_state=seed_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_ML_tran.to_excel(project_path+'/data/result/cox/df_1.7.2.2_after_ML_tran.xlsx')\n",
    "df_after_ML_test.to_excel(project_path+'/data/result/cox/df_1.7.2.2_after_ML_test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据集同分布检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surgery_result',\n",
       " 'AJCC_8',\n",
       " 'blood_type',\n",
       " 'MSKCC',\n",
       " 'HBcAb',\n",
       " 'surgery_plasm',\n",
       " 'HBsAg',\n",
       " 'surgery_bleeding',\n",
       " 'smoke',\n",
       " 'emaciation',\n",
       " 'drinking',\n",
       " 'PTCD_ERCP',\n",
       " 'biliary_disease',\n",
       " 'gender',\n",
       " 'LC',\n",
       " 'target_3Y_death']"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surgery_result\n",
      "AJCC_8\n",
      "blood_type\n",
      "MSKCC\n",
      "HBcAb\n",
      "surgery_plasm\n",
      "HBsAg\n",
      "surgery_bleeding\n",
      "smoke\n",
      "emaciation\n",
      "drinking\n",
      "PTCD_ERCP\n",
      "biliary_disease\n",
      "gender\n",
      "LC\n",
      "target_3Y_death\n"
     ]
    }
   ],
   "source": [
    "# 判断训练集与测试集是否独立同分布\n",
    "# 分类变量显著性分析\n",
    "from scipy.stats import chi2_contingency\n",
    "feature_list=[]\n",
    "y_list=[]\n",
    "t_list=[]\n",
    "p_list=[]\n",
    "sig_list=[]\n",
    "for i in discrete_col:\n",
    "    print(i)\n",
    "    if len(df_after_ML_tran[i].value_counts().to_list()) == len(df_after_ML_test[i].value_counts().to_list()):\n",
    "        result = chi2_contingency([df_after_ML_tran[i].value_counts().to_list(),\n",
    "                                   df_after_ML_test[i].value_counts().to_list()])\n",
    "        t,p=result[0:2]\n",
    "        t=round(t,2)\n",
    "        p=round(p,3)\n",
    "    else:\n",
    "        t=np.nan\n",
    "        p=np.nan\n",
    "    feature_list.append(i)\n",
    "    y_list.append('卡方检验')\n",
    "    t_list.append(t)\n",
    "    p_list.append(p)\n",
    "    if p <=0.05:\n",
    "        sig='显著'\n",
    "    else:\n",
    "        sig='不显著'\n",
    "    sig_list.append(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discrete_sig=pd.DataFrame(data={'feature':discrete_col,\n",
    "                                        'method':y_list,\n",
    "                                        't':t_list,\n",
    "                                        'p':p_list,\n",
    "                                        'result':sig_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>method</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>surgery_result</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.999</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AJCC_8</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blood_type</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.765</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSKCC</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.974</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HBcAb</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.769</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>surgery_plasm</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.735</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HBsAg</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.811</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>surgery_bleeding</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>smoke</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.982</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>emaciation</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.441</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>drinking</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.239</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PTCD_ERCP</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.370</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>biliary_disease</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.879</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gender</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.373</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LC</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.214</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>target_3Y_death</td>\n",
       "      <td>卡方检验</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.072</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature method     t      p result\n",
       "0     surgery_result   卡方检验  0.02  0.999    不显著\n",
       "1             AJCC_8   卡方检验   NaN    NaN    不显著\n",
       "2         blood_type   卡方检验  1.15  0.765    不显著\n",
       "3              MSKCC   卡方检验  0.05  0.974    不显著\n",
       "4              HBcAb   卡方检验  0.09  0.769    不显著\n",
       "5      surgery_plasm   卡方检验  4.38  0.735    不显著\n",
       "6              HBsAg   卡方检验  0.06  0.811    不显著\n",
       "7   surgery_bleeding   卡方检验   NaN    NaN    不显著\n",
       "8              smoke   卡方检验  0.00  0.982    不显著\n",
       "9         emaciation   卡方检验  1.64  0.441    不显著\n",
       "10          drinking   卡方检验  1.39  0.239    不显著\n",
       "11         PTCD_ERCP   卡方检验  3.14  0.370    不显著\n",
       "12   biliary_disease   卡方检验  0.02  0.879    不显著\n",
       "13            gender   卡方检验  0.79  0.373    不显著\n",
       "14                LC   卡方检验  1.55  0.214    不显著\n",
       "15   target_3Y_death   卡方检验  3.24  0.072    不显著"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_discrete_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discrete_sig.to_excel(project_path+'/data/result/cox/df_1.7.2.3_after_分类变量数据同分布检验.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tumor_CA19-9\n",
      "tumor_CA125\n",
      "tumor_size\n",
      "DB\n",
      "tumor_AFP\n",
      "tumor_CEA\n",
      "target_survival_month\n"
     ]
    }
   ],
   "source": [
    "# 连续变量显著性分析\n",
    "df_continuous_sig = sig_test(df_after_ML_tran,df_after_ML_test,continuous_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征</th>\n",
       "      <th>高剂量均值</th>\n",
       "      <th>低剂量均值</th>\n",
       "      <th>检验指标</th>\n",
       "      <th>t值</th>\n",
       "      <th>p值</th>\n",
       "      <th>显著性结果</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tumor_CA19-9</td>\n",
       "      <td>375.22</td>\n",
       "      <td>376.99</td>\n",
       "      <td>Mann-Whitney U检验</td>\n",
       "      <td>15882.0</td>\n",
       "      <td>0.466</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tumor_CA125</td>\n",
       "      <td>31.59</td>\n",
       "      <td>49.11</td>\n",
       "      <td>Mann-Whitney U检验</td>\n",
       "      <td>7058.0</td>\n",
       "      <td>0.465</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tumor_size</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.93</td>\n",
       "      <td>Mann-Whitney U检验</td>\n",
       "      <td>9141.0</td>\n",
       "      <td>0.410</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DB</td>\n",
       "      <td>147.15</td>\n",
       "      <td>135.15</td>\n",
       "      <td>Mann-Whitney U检验</td>\n",
       "      <td>11608.5</td>\n",
       "      <td>0.130</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tumor_AFP</td>\n",
       "      <td>8.35</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Mann-Whitney U检验</td>\n",
       "      <td>14773.0</td>\n",
       "      <td>0.444</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tumor_CEA</td>\n",
       "      <td>6.33</td>\n",
       "      <td>5.90</td>\n",
       "      <td>Mann-Whitney U检验</td>\n",
       "      <td>15320.0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>target_survival_month</td>\n",
       "      <td>24.98</td>\n",
       "      <td>20.10</td>\n",
       "      <td>Mann-Whitney U检验</td>\n",
       "      <td>8575.5</td>\n",
       "      <td>0.135</td>\n",
       "      <td>不显著</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      特征   高剂量均值   低剂量均值              检验指标       t值     p值  \\\n",
       "0           tumor_CA19-9  375.22  376.99  Mann-Whitney U检验  15882.0  0.466   \n",
       "1            tumor_CA125   31.59   49.11  Mann-Whitney U检验   7058.0  0.465   \n",
       "2             tumor_size    2.88    2.93  Mann-Whitney U检验   9141.0  0.410   \n",
       "3                     DB  147.15  135.15  Mann-Whitney U检验  11608.5  0.130   \n",
       "4              tumor_AFP    8.35    3.61  Mann-Whitney U检验  14773.0  0.444   \n",
       "5              tumor_CEA    6.33    5.90  Mann-Whitney U检验  15320.0  0.438   \n",
       "6  target_survival_month   24.98   20.10  Mann-Whitney U检验   8575.5  0.135   \n",
       "\n",
       "  显著性结果  \n",
       "0   不显著  \n",
       "1   不显著  \n",
       "2   不显著  \n",
       "3   不显著  \n",
       "4   不显著  \n",
       "5   不显著  \n",
       "6   不显著  "
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_continuous_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continuous_sig.to_excel(project_path+'/data/result/cox/df_1.7.2.3_after_连续变量数据同分布检验.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据统计分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类变量数据统计\n",
    "def stats_discrete(df_model,discrete_col):\n",
    "    # 求分类变量比例\n",
    "    df_discrete_stat=pd.DataFrame(columns=['变量名称','所有事件(%d)' % df_model.shape[0],'缺失率(%)'])\n",
    "    for i in discrete_col:\n",
    "        print(i)\n",
    "        # 缺失率\n",
    "        if df_model[i].isnull().sum()==0:\n",
    "            miss_rate='0%'\n",
    "        else:\n",
    "            miss_rate=df_model[i].isnull().sum()/df_model.shape[0]\n",
    "            miss_rate=\"%.2f%%\" % (miss_rate * 100)      # 百分数输出\n",
    "        df_discrete_stat.loc[df_discrete_stat.shape[0],['变量名称','缺失率(%)']]=[i+'，n(%)',miss_rate]\n",
    "\n",
    "        # 分类变量单独统计\n",
    "        name_list=[]\n",
    "        num_perc_list=[]\n",
    "        df_model_stat=df_model[df_model[i].notnull()].sort_values([i],ascending=True)\n",
    "\n",
    "        # 二分类还是多分类变量\n",
    "        if df_model_stat[i].nunique() <=2:\n",
    "            if re.match('gender|性别',i):\n",
    "                name_list=['男','女']\n",
    "            else:\n",
    "                name_list=['是','否']\n",
    "            for name,value in zip(name_list,[1,0]):\n",
    "                print(name)\n",
    "                num=df_model_stat[df_model_stat[i]==value].shape[0]\n",
    "                percent=num/df_model.shape[0]\n",
    "                percent=\"%.2f%%\" % (percent * 100)\n",
    "                num_percent=str(num)+'('+percent+')'\n",
    "                num_perc_list.append(num_percent)\n",
    "        else:\n",
    "            for value in sorted(df_model_stat[i].unique()):\n",
    "                print(value)\n",
    "                name_list.append(value)\n",
    "                num=df_model_stat[df_model_stat[i]==value].shape[0]\n",
    "                percent=num/df_model.shape[0]\n",
    "                percent=\"%.2f%%\" % (percent * 100)\n",
    "                num_percent=str(num)+'('+percent+')'\n",
    "                num_perc_list.append(num_percent)\n",
    "\n",
    "\n",
    "        df_temp = pd.DataFrame(data={'变量名称':name_list,\n",
    "                                     '所有事件(%d)' % df_model.shape[0]:num_perc_list})\n",
    "\n",
    "        df_discrete_stat=pd.concat([df_discrete_stat,df_temp],axis=0)\n",
    "        df_discrete_stat=df_discrete_stat.reset_index(drop=True)\n",
    "    return df_discrete_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surgery_result\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "AJCC_8\n",
      "0.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "blood_type\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "MSKCC\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "HBcAb\n",
      "是\n",
      "否\n",
      "surgery_plasm\n",
      "0.0\n",
      "200.0\n",
      "300.0\n",
      "400.0\n",
      "600.0\n",
      "800.0\n",
      "1000.0\n",
      "1400.0\n",
      "HBsAg\n",
      "是\n",
      "否\n",
      "surgery_bleeding\n",
      "5.0\n",
      "10.0\n",
      "50.0\n",
      "80.0\n",
      "100.0\n",
      "150.0\n",
      "200.0\n",
      "250.0\n",
      "300.0\n",
      "350.0\n",
      "400.0\n",
      "500.0\n",
      "600.0\n",
      "700.0\n",
      "750.0\n",
      "800.0\n",
      "1000.0\n",
      "1200.0\n",
      "1300.0\n",
      "1400.0\n",
      "1500.0\n",
      "1600.0\n",
      "2000.0\n",
      "smoke\n",
      "是\n",
      "否\n",
      "emaciation\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "drinking\n",
      "是\n",
      "否\n",
      "PTCD_ERCP\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "biliary_disease\n",
      "是\n",
      "否\n",
      "gender\n",
      "男\n",
      "女\n",
      "LC\n",
      "是\n",
      "否\n",
      "target_3Y_death\n",
      "是\n",
      "否\n",
      "surgery_result\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "AJCC_8\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "blood_type\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "MSKCC\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "HBcAb\n",
      "是\n",
      "否\n",
      "surgery_plasm\n",
      "0.0\n",
      "200.0\n",
      "400.0\n",
      "600.0\n",
      "800.0\n",
      "1000.0\n",
      "1200.0\n",
      "2800.0\n",
      "HBsAg\n",
      "是\n",
      "否\n",
      "surgery_bleeding\n",
      "20.0\n",
      "50.0\n",
      "100.0\n",
      "150.0\n",
      "200.0\n",
      "300.0\n",
      "350.0\n",
      "400.0\n",
      "450.0\n",
      "500.0\n",
      "600.0\n",
      "700.0\n",
      "800.0\n",
      "1200.0\n",
      "4000.0\n",
      "smoke\n",
      "是\n",
      "否\n",
      "emaciation\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "drinking\n",
      "是\n",
      "否\n",
      "PTCD_ERCP\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "biliary_disease\n",
      "是\n",
      "否\n",
      "gender\n",
      "男\n",
      "女\n",
      "LC\n",
      "是\n",
      "否\n",
      "target_3Y_death\n",
      "是\n",
      "否\n"
     ]
    }
   ],
   "source": [
    "df_discrete_stat_tran=stats_discrete(df_after_ML_tran,discrete_col)\n",
    "df_discrete_stat_tran.to_excel(project_path+'/data/result/cox/df_1.7.2.4_after_分类_tran_数据统计.xlsx')\n",
    "df_discrete_stat_test=stats_discrete(df_after_ML_test,discrete_col)\n",
    "df_discrete_stat_test.to_excel(project_path+'/data/result/cox/df_1.7.2.4_after_分类_test_数据统计.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续变量数据统计\n",
    "def stats_continuous(df_model,continuous_col):\n",
    "    # 统计全变量体系各变量的平均数、上下四分位数、缺失率\n",
    "    feature_quarter_list=[]\n",
    "    mean_quarter_list=[]\n",
    "    feature_std_list=[]\n",
    "    mean_std_list=[]\n",
    "    miss_list=[]\n",
    "    for i in continuous_col:\n",
    "        # 计算上下四分位、均值、标准差\n",
    "        try:\n",
    "            data = df_model[i].astype('float')\n",
    "            stat_result = pd.DataFrame(data.describe())\n",
    "            mean_value=stat_result.loc['mean',i]\n",
    "            up_quarter=stat_result.loc['25%',i]\n",
    "            down_quarter=stat_result.loc['75%',i]\n",
    "            std_value=stat_result.loc['std',i]\n",
    "        except:\n",
    "            mean_value=np.nan\n",
    "            up_quarter=np.nan\n",
    "            down_quarter=np.nan\n",
    "        # 计算缺失率\n",
    "        if df_model[i].isnull().sum()==0:\n",
    "            miss_rate='0%'\n",
    "        else:\n",
    "            miss_rate=df_model[i].isnull().sum()/df_model.shape[0]\n",
    "            miss_rate=\"%.2f%%\" % (miss_rate * 100)      # 百分数输出\n",
    "        miss_list.append(miss_rate)\n",
    "        # mean(quarter)\n",
    "        feature_quarter_list.append(i+'，mean（IQR）')\n",
    "        mean_quarter_list.append('%.2f(%.2f-%.2f)' % (mean_value,up_quarter,down_quarter))\n",
    "        # mean(std)\n",
    "        feature_std_list.append(i+'，mean±std')\n",
    "        mean_std_list.append('%.2f±%.2f' % (mean_value,std_value))\n",
    "\n",
    "    df_continuous_quarter=pd.DataFrame(data={'特征':feature_quarter_list,\n",
    "                            'mean_quarter_list':mean_quarter_list,\n",
    "                            'miss_list':miss_list})\n",
    "    df_continuous_std=pd.DataFrame(data={'特征':feature_std_list,\n",
    "                            'mean_std_list':mean_std_list,\n",
    "                            'miss_list':miss_list})\n",
    "    return df_continuous_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continuous_stat_tran=stats_continuous(df_after_ML_tran,continuous_col)\n",
    "df_continuous_stat_tran.to_excel(project_path+'/data/result/cox/df_1.7.2.4_after_连续_tran_数据统计.xlsx')\n",
    "df_continuous_stat_test=stats_continuous(df_after_ML_test,continuous_col)\n",
    "df_continuous_stat_test.to_excel(project_path+'/data/result/cox/df_1.7.2.4_after_连续_test_数据统计.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理Ml变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每列缺失率: surgery_result 10.20%\n",
      "每列缺失率: AJCC_8 6.87%\n",
      "每列缺失率: blood_type 71.18%\n",
      "每列缺失率: tumor_CA19-9 1.33%\n",
      "每列缺失率: MSKCC 7.54%\n",
      "每列缺失率: tumor_CA125 34.15%\n",
      "每列缺失率: HBcAb 15.30%\n",
      "每列缺失率: surgery_plasm 10.42%\n",
      "每列缺失率: HBsAg 9.53%\n",
      "每列缺失率: surgery_bleeding 9.76%\n",
      "每列缺失率: tumor_size 22.84%\n",
      "每列缺失率: smoke 0.00%\n",
      "每列缺失率: emaciation 14.19%\n",
      "每列缺失率: drinking 4.88%\n",
      "每列缺失率: DB 12.20%\n",
      "每列缺失率: tumor_AFP 3.99%\n",
      "每列缺失率: tumor_CEA 2.44%\n",
      "每列缺失率: PTCD_ERCP 5.76%\n",
      "每列缺失率: biliary_disease 0.44%\n",
      "每列缺失率: gender 0.22%\n",
      "每列缺失率: LC 25.28%\n",
      "每列缺失率: target_3Y_death 25.06%\n",
      "每列缺失率: target_survival_month 25.06%\n"
     ]
    }
   ],
   "source": [
    "# 查看缺失率\n",
    "for i in df_dataset_cox_after_importance.columns:\n",
    "    percent_col= df_dataset_cox_after_importance[i].isnull().sum()/df_dataset_cox_after_importance.shape[0]\n",
    "    percent_col=\"%.2f%%\" % (percent_col * 100)\n",
    "    print('每列缺失率:',i,percent_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除缺失超过20%的变量\n",
    "df_after_ML_tran_cox=df_after_ML_tran.drop(['blood_type','tumor_CA125','tumor_size','LC'],axis=1)\n",
    "df_after_ML_test_cox=df_after_ML_test.drop(['blood_type','tumor_CA125','tumor_size','LC'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 19)"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_ML_tran_cox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剔除缺失值\n",
    "for i in df_after_ML_tran_cox.columns:\n",
    "    df_after_ML_tran_cox = df_after_ML_tran_cox[df_after_ML_tran_cox[i].notnull()]\n",
    "    df_after_ML_test_cox = df_after_ML_test_cox[df_after_ML_test_cox[i].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 19)"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_ML_tran_cox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类不平衡变量\n",
    "for i in df_dataset_cox_after_importance.columns:\n",
    "    if df_dataset_cox_after_importance[i].nunique() < 2:\n",
    "        print(i)\n",
    "        del df_after_ML_tran_cox[i]\n",
    "        del df_after_ML_test_cox[i]\n",
    "        continue\n",
    "    if df_dataset_cox_after_importance[i].nunique() == 2:\n",
    "        # 如果分类变量中某一变量的占比超过90%，则删除该指标\n",
    "        num_1 = df_dataset_cox_after_importance[i].value_counts()  # df一列中不同变量的数目\n",
    "        num_2 = num_1.div(df_dataset_cox_after_importance.shape[0])  # div除法，所有元素都除以相同数值\n",
    "        num_3 = num_2.max()  # 取出最大值\n",
    "        if num_3 >= 0.9:\n",
    "            print(i)\n",
    "            del df_after_ML_tran_cox[i]\n",
    "            del df_after_ML_test_cox[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['surgery_result', 'AJCC_8', 'tumor_CA19-9', 'MSKCC', 'HBcAb',\n",
       "       'surgery_plasm', 'HBsAg', 'surgery_bleeding', 'smoke', 'emaciation',\n",
       "       'drinking', 'DB', 'tumor_AFP', 'tumor_CEA', 'PTCD_ERCP',\n",
       "       'biliary_disease', 'gender', 'target_3Y_death',\n",
       "       'target_survival_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_ML_tran_cox.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_ML_tran_cox.to_excel(project_path+'/data/result/cox/df_1.7.2.5_after_ML_tran_cox.xlsx')\n",
    "df_after_ML_test_cox.to_excel(project_path+'/data/result/cox/df_1.7.2.5_after_ML_test_cox.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理全部变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每列缺失率: id 0.00%\n",
      "每列缺失率: sampling 0.00%\n",
      "每列缺失率: gender 0.22%\n",
      "每列缺失率: age 0.22%\n",
      "每列缺失率: height 31.93%\n",
      "每列缺失率: weight 31.93%\n",
      "每列缺失率: BMI 31.93%\n",
      "每列缺失率: jaundice 0.00%\n",
      "每列缺失率: emaciation 14.19%\n",
      "每列缺失率: breath_disease 0.22%\n",
      "每列缺失率: cardio_disease 0.22%\n",
      "每列缺失率: nbdd 0.22%\n",
      "每列缺失率: urinary_disease 0.67%\n",
      "每列缺失率: endocrine_disease 0.44%\n",
      "每列缺失率: biliary_disease 0.44%\n",
      "每列缺失率: other_disease 0.89%\n",
      "每列缺失率: smoke 0.00%\n",
      "每列缺失率: drinking 4.88%\n",
      "每列缺失率: family_history 1.55%\n",
      "每列缺失率: blood_type 71.18%\n",
      "每列缺失率: WBC 12.42%\n",
      "每列缺失率: HGB 13.08%\n",
      "每列缺失率: PLT 15.30%\n",
      "每列缺失率: TB 1.11%\n",
      "每列缺失率: DB 12.20%\n",
      "每列缺失率: TP 16.63%\n",
      "每列缺失率: ALB 3.77%\n",
      "每列缺失率: LG 66.30%\n",
      "每列缺失率: AG 32.59%\n",
      "每列缺失率: PAB 23.06%\n",
      "每列缺失率: ALT 11.97%\n",
      "每列缺失率: AST 18.63%\n",
      "每列缺失率: GT 28.60%\n",
      "每列缺失率: ALP 30.60%\n",
      "每列缺失率: tumor_AFP 3.99%\n",
      "每列缺失率: tumor_CEA 2.44%\n",
      "每列缺失率: tumor_CA19-9 1.33%\n",
      "每列缺失率: tumor_CA125 34.15%\n",
      "每列缺失率: TF 78.71%\n",
      "每列缺失率: tumor_size 22.84%\n",
      "每列缺失率: HBsAg 9.53%\n",
      "每列缺失率: HBeAg 15.08%\n",
      "每列缺失率: HBeAb 15.08%\n",
      "每列缺失率: HBcAb 15.30%\n",
      "每列缺失率: HCVAb 15.96%\n",
      "每列缺失率: LC 25.28%\n",
      "每列缺失率: AJCC_8 6.87%\n",
      "每列缺失率: Gazzaniga_T 7.54%\n",
      "每列缺失率: MSKCC 7.54%\n",
      "每列缺失率: Blumgart_T 7.54%\n",
      "每列缺失率: Bismuth_C 7.10%\n",
      "每列缺失率: PTCD_ERCP 5.76%\n",
      "每列缺失率: 手术日期 5.54%\n",
      "每列缺失率: surgery_bleeding 9.76%\n",
      "每列缺失率: surgery_CRCS 9.98%\n",
      "每列缺失率: surgery_plasm 10.42%\n",
      "每列缺失率: surgery_CP 26.83%\n",
      "每列缺失率: surgery_result 10.20%\n",
      "每列缺失率: gene_mutation 0.00%\n",
      "每列缺失率: gene_MSI 94.01%\n",
      "每列缺失率: TMB 98.45%\n",
      "每列缺失率: IHC_cdx2 78.71%\n",
      "每列缺失率: IHC_cea 86.70%\n",
      "每列缺失率: IHC_ck5 79.82%\n",
      "每列缺失率: IHC_ck7 76.27%\n",
      "每列缺失率: IHC_ck19 75.83%\n",
      "每列缺失率: IHC_ck20 78.27%\n",
      "每列缺失率: IHC_muc1 77.61%\n",
      "每列缺失率: IHC_moc31 88.25%\n",
      "每列缺失率: IHC_pd1 98.00%\n",
      "每列缺失率: IHC_pdl1 96.67%\n",
      "每列缺失率: target_death 0.00%\n",
      "每列缺失率: target_survival_month 25.06%\n",
      "每列缺失率: target_1Y_death 25.06%\n",
      "每列缺失率: target_3Y_death 25.06%\n"
     ]
    }
   ],
   "source": [
    "# 查看缺失率\n",
    "for i in df_dataset_cox_after.columns:\n",
    "    percent_col= df_dataset_cox_after[i].isnull().sum()/df_dataset_cox_after.shape[0]\n",
    "    percent_col=\"%.2f%%\" % (percent_col * 100)\n",
    "    print('每列缺失率:',i,percent_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451, 75)"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_cox_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除无效变量\n",
    "df_dataset_cox_after=df_dataset_cox_after.drop(['id','sampling','手术日期','target_1Y_death','target_death'],axis=1)\n",
    "# 删除缺失超过20%的变量\n",
    "df_dataset_cox_after=df_dataset_cox_after.drop(['height','weight','BMI','blood_type','LG','AG','PAB','GT','ALP','tumor_CA125',\n",
    "                                                'TF','tumor_size','LC','surgery_CP','gene_MSI','TMB','IHC_cdx2','IHC_cea',\n",
    "                                               'IHC_ck5','IHC_ck7','IHC_ck19','IHC_ck20','IHC_muc1','IHC_moc31','IHC_pd1',\n",
    "                                               'IHC_pdl1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breath_disease\n",
      "urinary_disease\n",
      "family_history\n"
     ]
    }
   ],
   "source": [
    "# 分类不平衡变量\n",
    "for i in df_dataset_cox_after.columns:\n",
    "    if df_dataset_cox_after[i].nunique() < 2:\n",
    "        print(i)\n",
    "        del df_dataset_cox_after[i]\n",
    "        continue\n",
    "    if df_dataset_cox_after[i].nunique() == 2:\n",
    "        # 如果分类变量中某一变量的占比超过90%，则删除该指标\n",
    "        num_1 = df_dataset_cox_after[i].value_counts()  # df一列中不同变量的数目\n",
    "        num_2 = num_1.div(df_dataset_cox_after.shape[0])  # div除法，所有元素都除以相同数值\n",
    "        num_3 = num_2.max()  # 取出最大值\n",
    "        if num_3 >= 0.9:\n",
    "            print(i)\n",
    "            del df_dataset_cox_after[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'age', 'jaundice', 'emaciation', 'cardio_disease', 'nbdd',\n",
       "       'endocrine_disease', 'biliary_disease', 'other_disease', 'smoke',\n",
       "       'drinking', 'WBC', 'HGB', 'PLT', 'TB', 'DB', 'TP', 'ALB', 'ALT', 'AST',\n",
       "       'tumor_AFP', 'tumor_CEA', 'tumor_CA19-9', 'HBsAg', 'HBeAg', 'HBeAb',\n",
       "       'HBcAb', 'HCVAb', 'AJCC_8', 'Gazzaniga_T', 'MSKCC', 'Blumgart_T',\n",
       "       'Bismuth_C', 'PTCD_ERCP', 'surgery_bleeding', 'surgery_CRCS',\n",
       "       'surgery_plasm', 'surgery_result', 'gene_mutation',\n",
       "       'target_survival_month', 'target_3Y_death'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_cox_after.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部数据集划分\n",
    "df_after_all_tran_cox, df_after_all_test_cox = train_test_split(df_dataset_cox_after, test_size=0.2, random_state=seed_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剔除缺失值\n",
    "for i in df_dataset_cox_after.columns:\n",
    "    df_after_all_tran_cox = df_after_all_tran_cox[df_after_all_tran_cox[i].notnull()]\n",
    "    df_after_all_test_cox = df_after_all_test_cox[df_after_all_test_cox[i].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451, 41)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_cox_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_all_tran_cox.to_excel(project_path+'/data/result/cox/df_1.7.2.6_after_all_tran_cox.xlsx')\n",
    "df_after_all_test_cox.to_excel(project_path+'/data/result/cox/df_1.7.2.6_after_all_test_cox.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R: ML + backward stepwise for postoperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n手术结果\\t0.419\\nAJCC_8分期\\t0.760\\n肿瘤指标CA19-9\\t0.001\\n肿瘤指标CEA\\t0.053\\n'"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "手术结果\t0.419\n",
    "AJCC_8分期\t0.760\n",
    "肿瘤指标CA19-9\t0.001\n",
    "肿瘤指标CEA\t0.053\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R: 模型筛选能力对比，c_index和BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n全部变量\\n全部变量+backward stepwise\\n机器学习变量\\n机器学习变量 + backward stepwise\\n'"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "全部变量\n",
    "全部变量+backward stepwise\n",
    "机器学习变量\n",
    "机器学习变量 + backward stepwise\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### python: 计算c_index和BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 术后预后3年生存期\n",
    "\n",
    "# # 删除1年生存期的空值\n",
    "# df_dataset_cox_after_3Y=df_dataset_cox_after_importance[df_dataset_cox_after_importance.target_3Y_survival.notnull()]\n",
    "# # 删除无用变量\n",
    "# df_dataset_cox_after_3Y=df_dataset_cox_after_3Y.drop(['target_1Y_survival','target_1Y_death', 'target_5Y_death',\n",
    "#                                             'LC','target_5Y_survival', 'target_8Y_death', 'target_8Y_survival'],axis=1)\n",
    "\n",
    "# df_dataset_cox_after_3Y.shape\n",
    "\n",
    "# df_dataset_cox_after_3Y.columns\n",
    "\n",
    "# # 剔除缺失值\n",
    "# for i in df_dataset_cox_after_3Y.columns:\n",
    "#     df_dataset_cox_after_3Y = df_dataset_cox_after_3Y[df_dataset_cox_after_3Y[i].notnull()]\n",
    "\n",
    "# df_dataset_cox_after_3Y.sampling.value_counts()\n",
    "\n",
    "# df_dataset_cox_after_3Y.to_excel(project_path+'/data/result/cox/df_1.7.1_dataset_cox_after_3Y_ML.xlsx')\n",
    "\n",
    "# # 计算c_index of ML\n",
    "# from lifelines import CoxPHFitter\n",
    "# from lifelines.utils import concordance_index\n",
    "\n",
    "# cph = CoxPHFitter()\n",
    "# cph.fit(df_dataset_cox_after_3Y, duration_col='target_3Y_survival',event_col='target_3Y_death',cluster_col='sampling',show_progress=True)\n",
    "# c_index_3Y = cph.concordance_index_\n",
    "# print(c_index_3Y)\n",
    "\n",
    "# # 术后3年SPSS：backward再次筛选\n",
    "# col_backward_3Y=['surgery_result','AJCC_8','tumor_CA199','biliary_disease']\n",
    "# col_backward_3Y.insert(0,'sampling')\n",
    "# col_backward_3Y.extend(['target_3Y_death','target_3Y_survival'])\n",
    "# df_dataset_cox_after_3Y_backward=df_dataset_cox_after_3Y[col_backward_3Y]\n",
    "\n",
    "# # 计算c_index of ML+SPSS\n",
    "# from lifelines import CoxPHFitter\n",
    "# from lifelines.utils import concordance_index\n",
    "\n",
    "# cph = CoxPHFitter()\n",
    "# cph.fit(df_dataset_cox_after_3Y_backward, duration_col='target_3Y_survival',event_col='target_3Y_death',cluster_col='sampling',show_progress=True)\n",
    "# c_index_3Y_backward = cph.concordance_index_\n",
    "# print(c_index_3Y_backward)\n",
    "\n",
    "# # 计算3年生存期的BS\n",
    "# from sksurv.metrics import brier_score\n",
    "# from sksurv.datasets import load_gbsg2\n",
    "# from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "# from sksurv.metrics import brier_score\n",
    "# from sksurv.preprocessing import OneHotEncoder\n",
    "\n",
    "# #### 术后预后3年生存期\n",
    "\n",
    "# df_dataset_cox_after_3Y =pd.read_excel(project_path +'/data/result/cox/df_1.7.1_dataset_cox_after_3Y_ML.xlsx')\n",
    "# if 'Unnamed: 0' in df_dataset_cox_after_3Y.columns:\n",
    "#     df_dataset_cox_after_3Y = df_dataset_cox_after_3Y.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# # 转换成boolean值\n",
    "# df_dataset_cox_after_3Y['target_death_binary']=df_dataset_cox_after_3Y.target_3Y_death.astype(bool)\n",
    "# # 构建BStrain_data和test_data\n",
    "# bs_y = [(d,s) for d,s in zip(df_dataset_cox_after_3Y.target_death_binary,df_dataset_cox_after_3Y.target_3Y_survival)]\n",
    "# bs_y=np.array(bs_y,dtype=[('cens', '?'), ('time', '<f8')])\n",
    "\n",
    "# df_dataset_cox_after_3Y.columns\n",
    "\n",
    "# # 术后3年BS_after_3Y\n",
    "# bs_x_after_3Y = df_dataset_cox_after_3Y.drop(['target_3Y_death','target_3Y_survival','sampling'],axis=1)\n",
    "# est_after_3Y = CoxPHSurvivalAnalysis(ties=\"efron\").fit(bs_x_after_3Y, bs_y)\n",
    "# survs_after_3Y = est_after_3Y.predict_survival_function(bs_x_after_3Y)\n",
    "# preds_after_3Y = [fn(10) for fn in survs_after_3Y]\n",
    "# times, bs_after_3Y = brier_score(bs_y, bs_y, preds_after_3Y, 10)\n",
    "# print(bs_after_3Y)\n",
    "\n",
    "# # 术后3年SPSS：backward再次筛选\n",
    "# bs_x_after_3Y_backward=bs_x_after_3Y[['surgery_result','AJCC_8','tumor_CA199','biliary_disease']]\n",
    "# est_after_3Y_backward=CoxPHSurvivalAnalysis(ties=\"efron\").fit(bs_x_after_3Y_backward, bs_y)\n",
    "# survs_after_3Y_backward = est_after_3Y_backward.predict_survival_function(bs_x_after_3Y_backward)\n",
    "# preds_after_3Y_backward = [fn(10) for fn in survs_after_3Y_backward]\n",
    "# times, bs_after_3Y_backward = brier_score(bs_y, bs_y, preds_after_3Y_backward, 10)\n",
    "# print(bs_after_3Y_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 术后风险评分模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPSS-cox回归分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nML+SPSS:backward筛选的变量计算风险评分：\\n1年生存期cox系数：\\n    surgery_result 0.734\\n    AJCC_8 0.879\\n    tumor_CA199 0.001\\n    smoke 0.607\\n    emaciation -0.756\\n3年生存期cox系数：\\n    surgery_result 0.486\\n    AJCC_8 0.561\\n    tumor_CA199 0.001\\n    biliary_disease 0.491\\n5年生存期cox系数\\n    surgery_result 0.574\\n    AJCC_8 0.486\\n    tumor_CA199 0.001\\n    biliary_disease 0.436\\n8年生存期cox系数\\n    surgery_result 0.606\\n    AJCC_8 0.482\\n    tumor_CA199 0.001\\n    biliary_disease 0.444\\n'"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ML+SPSS:backward筛选的变量计算风险评分：\n",
    "1年生存期cox系数：\n",
    "    surgery_result 0.734\n",
    "    AJCC_8 0.879\n",
    "    tumor_CA199 0.001\n",
    "    smoke 0.607\n",
    "    emaciation -0.756\n",
    "3年生存期cox系数：\n",
    "    surgery_result 0.486\n",
    "    AJCC_8 0.561\n",
    "    tumor_CA199 0.001\n",
    "    biliary_disease 0.491\n",
    "5年生存期cox系数\n",
    "    surgery_result 0.574\n",
    "    AJCC_8 0.486\n",
    "    tumor_CA199 0.001\n",
    "    biliary_disease 0.436\n",
    "8年生存期cox系数\n",
    "    surgery_result 0.606\n",
    "    AJCC_8 0.482\n",
    "    tumor_CA199 0.001\n",
    "    biliary_disease 0.444\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1年生存期cox-LR回归\n",
    "# col_cox_select_1Y=['surgery_result','AJCC_8','tumor_CA199','smoke','emaciation']\n",
    "# # 3年生存期cox-LR回归\n",
    "# col_cox_select_3Y=['surgery_result','AJCC_8','tumor_CA199','biliary_disease']\n",
    "# # 5年生存期cox-LR回归\n",
    "# col_cox_select_5Y=['surgery_result','AJCC_8','tumor_CA199','biliary_disease']\n",
    "# # 8年生存期cox-LR回归\n",
    "# col_cox_select_8Y=['surgery_result','AJCC_8','tumor_CA199','biliary_disease']\n",
    "# # 合并风险模型变量\n",
    "# col_cox_select=list(set(col_cox_select_1Y + col_cox_select_3Y + col_cox_select_5Y + col_cox_select_8Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surgery_result',\n",
       " 'tumor_CA19-9',\n",
       " 'tumor_CEA',\n",
       " 'surgery_plasm',\n",
       " 'biliary_disease',\n",
       " 'PLT',\n",
       " 'AJCC_8',\n",
       " 'emaciation',\n",
       " 'smoke']"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_cox_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute risk-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_risk_after =pd.read_excel(project_path +'/data/processed_data/df_3.1.1_术后预后cox数据集.xlsx')\n",
    "if 'Unnamed: 0' in df_model_risk_after.columns:\n",
    "    df_model_risk_after = df_model_risk_after.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451, 75)"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_risk_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# 计算风险评分\n",
    "df_model_risk_after=df_model_risk_after.reset_index(drop=True)\n",
    "# 缺失项补充为0\n",
    "col_cox_select=['surgery_result','tumor_CA19-9','tumor_CEA','surgery_plasm','biliary_disease','PLT','AJCC_8','emaciation','smoke']\n",
    "df_model_risk_after[col_cox_select]=df_model_risk_after[col_cox_select].fillna(0)\n",
    "for i in range(df_model_risk_after.shape[0]):\n",
    "    surgery_result=float(df_model_risk_after.loc[i,'surgery_result'])\n",
    "    ca199=float(df_model_risk_after.loc[i,'tumor_CA19-9'])\n",
    "    ajcc=float(df_model_risk_after.loc[i,'AJCC_8'])\n",
    "    biliary = float(df_model_risk_after.loc[i,'biliary_disease'])\n",
    "    emaciation=float(df_model_risk_after.loc[i,'emaciation'])\n",
    "    smoke=float(df_model_risk_after.loc[i,'smoke'])\n",
    "    plt_value=float(df_model_risk_after.loc[i,'PLT'])\n",
    "    surgery_plasm=float(df_model_risk_after.loc[i,'surgery_plasm'])\n",
    "    cea=float(df_model_risk_after.loc[i,'tumor_CEA'])\n",
    "\n",
    "\n",
    "#     # 术后1年生存期risk_score计算\n",
    "#     df_model_risk_after.loc[i,'risk_score_1Y']=0.734*surgery_result+0.001*ca199+0.879*ajcc+0.607*smoke-0.756*emaciation\n",
    "#     # 术后3年生存期risk_score计算\n",
    "#     df_model_risk_after.loc[i,'risk_score_3Y']=0.486*surgery_result+0.001*ca199+0.561*ajcc+0.491*biliary\n",
    "    # 术后5年生存期risk_score计算\n",
    "    df_model_risk_after.loc[i,'risk_score_3Y']=0.419*surgery_result+0.76*ajcc+0.001*ca199+0.053*cea\n",
    "#     # 术后8年生存期risk_score计算\n",
    "#     df_model_risk_after.loc[i,'risk_score_8Y']=0.606*surgery_result+0.001*ca199+0.482*ajcc+0.444*biliary\n",
    "    # 现有方法3年生存期risk_score计算\n",
    "    if plt_value <=0:\n",
    "        df_model_risk_after.loc[i,'risk_score_existing']=ajcc*0.58+0.59*surgery_result+0.0004*surgery_plasm\n",
    "    else:\n",
    "        df_model_risk_after.loc[i,'risk_score_existing']=math.log(plt_value)*0.002+ajcc*0.58+0.59*surgery_result+\\\n",
    "                                                    0.0004*surgery_plasm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451, 77)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_risk_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    451.000000\n",
       "mean       3.221222\n",
       "std        1.611756\n",
       "min        0.000000\n",
       "25%        2.305450\n",
       "50%        3.030600\n",
       "75%        3.961335\n",
       "max       20.344000\n",
       "Name: risk_score_3Y, dtype: float64"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_risk_after.risk_score_3Y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    451.000000\n",
       "mean       2.409457\n",
       "std        1.106319\n",
       "min        0.000000\n",
       "25%        1.761200\n",
       "50%        2.340665\n",
       "75%        2.910647\n",
       "max        5.011693\n",
       "Name: risk_score_existing, dtype: float64"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_risk_after.risk_score_existing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_risk_after.to_excel(project_path+'/data/result/cox/df_1.8.2_risk_score_after.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 术后分期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nR语言：风险评分分布直方图\\n根据风险评分分布直方图和中位数、四分卫点划分病理分期\\n'"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "R语言：风险评分分布直方图\n",
    "根据风险评分分布直方图和中位数、四分卫点划分病理分期\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 术后3年生存期风险评分分期\n",
    "df_model_risk_after['staging_after_3Y']=df_model_risk_after.risk_score_3Y.apply(lambda x: 1 if x<=2 else\n",
    "                                                                         2 if 2<x<=3 else\n",
    "                                                                         3 if 3<x<=4 else\n",
    "                                                                         4 if 4<x<=5 else\n",
    "                                                                         5 if x>5 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现有方法3年生存期风险评分分期\n",
    "df_model_risk_after['staging_after_existing']=df_model_risk_after.risk_score_existing.apply(lambda x: 1 if x<=2 else\n",
    "                                                                         2 if 2<x<=3 else\n",
    "                                                                         3 if 3<x<=4 else\n",
    "                                                                         4 if 4<x<=5 else\n",
    "                                                                         5 if x>5 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    184\n",
       "3    123\n",
       "4     66\n",
       "5     41\n",
       "1     37\n",
       "Name: staging_after_3Y, dtype: int64"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_risk_after.staging_after_3Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    179\n",
       "1    173\n",
       "4     61\n",
       "3     36\n",
       "5      2\n",
       "Name: staging_after_existing, dtype: int64"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_risk_after.staging_after_existing.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_risk_after.to_excel(project_path+'/data/result/cox/df_1.8.3_术后分期.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451, 79)"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_risk_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 划分训练集和测试集，比例为8:2\n",
    "\n",
    "df_after_staging_tran, df_after_staging_test = train_test_split(df_model_risk_after, test_size=0.2, random_state=seed_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 79)\n",
      "(91, 79)\n"
     ]
    }
   ],
   "source": [
    "print(df_after_staging_tran.shape)\n",
    "print(df_after_staging_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_staging_tran.to_excel(project_path+'/data/result/cox/df_1.8.3_术后分期_训练集.xlsx')\n",
    "df_after_staging_test.to_excel(project_path+'/data/result/cox/df_1.8.3_术后分期_测试集.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R语言: 风险评分直方画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R语言：术后分期生存分析图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R语言：术后分期比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python比较术后分期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sampling', 'gender', 'age', 'height', 'weight', 'BMI',\n",
       "       'jaundice', 'emaciation', 'breath_disease', 'cardio_disease', 'nbdd',\n",
       "       'urinary_disease', 'endocrine_disease', 'biliary_disease',\n",
       "       'other_disease', 'smoke', 'drinking', 'family_history', 'blood_type',\n",
       "       'WBC', 'HGB', 'PLT', 'TB', 'DB', 'TP', 'ALB', 'LG', 'AG', 'PAB', 'ALT',\n",
       "       'AST', 'GT', 'ALP', 'tumor_AFP', 'tumor_CEA', 'tumor_CA19-9',\n",
       "       'tumor_CA125', 'TF', 'tumor_size', 'HBsAg', 'HBeAg', 'HBeAb', 'HBcAb',\n",
       "       'HCVAb', 'LC', 'AJCC_8', 'Gazzaniga_T', 'MSKCC', 'Blumgart_T',\n",
       "       'Bismuth_C', 'PTCD_ERCP', '手术日期', 'surgery_bleeding', 'surgery_CRCS',\n",
       "       'surgery_plasm', 'surgery_CP', 'surgery_result', 'gene_mutation',\n",
       "       'gene_MSI', 'TMB', 'IHC_cdx2', 'IHC_cea', 'IHC_ck5', 'IHC_ck7',\n",
       "       'IHC_ck19', 'IHC_ck20', 'IHC_muc1', 'IHC_moc31', 'IHC_pd1', 'IHC_pdl1',\n",
       "       'target_death', 'target_survival_month', 'target_1Y_death',\n",
       "       'target_3Y_death', 'risk_score_3Y', 'risk_score_existing',\n",
       "       'staging_after_3Y', 'staging_after_existing'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_risk_after.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lifelines import CoxPHFitter\n",
    "# from lifelines.utils import concordance_index\n",
    "# # 计算1年生存期分期的c_index_us\n",
    "# df_C_index_us = df_model_risk_after[['staging_after_1Y','target_1Y_death','target_1Y_survival']]\n",
    "# df_C_index_us = df_C_index_us[df_C_index_us.target_1Y_survival.notnull()]\n",
    "# cph = CoxPHFitter()\n",
    "# cph.fit(df_C_index_us, duration_col='target_1Y_survival', event_col='target_1Y_death', show_progress=True)\n",
    "# C_index_us_1Y = cph.concordance_index_\n",
    "# print(C_index_us_1Y)\n",
    "\n",
    "# # 计算3年生存期分期的c_index_us\n",
    "# df_C_index_us = df_model_risk_after[['staging_after_3Y','target_3Y_death','target_3Y_survival']]\n",
    "# df_C_index_us = df_C_index_us[df_C_index_us.target_3Y_survival.notnull()]\n",
    "# cph = CoxPHFitter()\n",
    "# cph.fit(df_C_index_us, duration_col='target_3Y_survival', event_col='target_3Y_death', show_progress=True)\n",
    "# C_index_us_3Y = cph.concordance_index_\n",
    "# print(C_index_us_3Y)\n",
    "\n",
    "# # 计算5年生存期分期的c_index_us\n",
    "# df_C_index_us = df_model_risk_after[['staging_after_5Y','target_5Y_death','target_5Y_survival']]\n",
    "# df_C_index_us = df_C_index_us[df_C_index_us.target_5Y_survival.notnull()]\n",
    "# cph = CoxPHFitter()\n",
    "# cph.fit(df_C_index_us, duration_col='target_5Y_survival', event_col='target_5Y_death', show_progress=True)\n",
    "# C_index_us_5Y = cph.concordance_index_\n",
    "# print(C_index_us_5Y)\n",
    "\n",
    "# # 计算8年生存期分期的c_index_us\n",
    "# df_C_index_us = df_model_risk_after[['staging_after_8Y','target_8Y_death','target_8Y_survival']]\n",
    "# df_C_index_us = df_C_index_us[df_C_index_us.target_8Y_survival.notnull()]\n",
    "# cph = CoxPHFitter()\n",
    "# cph.fit(df_C_index_us, duration_col='target_8Y_survival', event_col='target_8Y_death', show_progress=True)\n",
    "# C_index_us_8Y = cph.concordance_index_\n",
    "# print(C_index_us_8Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python: sksurv计算BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sksurv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-525-10ccf9004a4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msksurv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrier_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msksurv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_gbsg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msksurv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCoxPHSurvivalAnalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msksurv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrier_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msksurv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sksurv'"
     ]
    }
   ],
   "source": [
    "from sksurv.metrics import brier_score\n",
    "from sksurv.datasets import load_gbsg2\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import brier_score\n",
    "from sksurv.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_bs=pd.read_excel(project_path+'/data/result/modeling/df_1.7.2_术后分期.xlsx')\n",
    "if 'Unnamed: 0' in df_model_bs.columns:\n",
    "    df_model_bs = df_model_bs.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_bs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除分期缺失值\n",
    "col_bs_select=['staging_after_1Y','staging_after_3Y','staging_after_5Y','staging_after_8Y','AJCC_8','Gazzaniga_T','MSKCC','Blumgart_T']\n",
    "for i in col_bs_select:\n",
    "    df_model_bs=df_model_bs[df_model_bs[i].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_bs[col_bs_select].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3Y--BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 术后预后3年生存期BS，生成布尔型True、False\n",
    "df_model_bs_3Y=df_model_bs[df_model_bs.target_3Y_survival.notnull()]\n",
    "df_model_bs_3Y['target_death_binary']=df_model_bs_3Y.target_3Y_death.astype(bool)\n",
    "# 构建BStrain_data和test_data\n",
    "bs_y = [(d,s) for d,s in zip(df_model_bs_3Y.target_death_binary,df_model_bs_3Y.target_3Y_survival)]\n",
    "bs_y=np.array(bs_y,dtype=[('cens', '?'), ('time', '<f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 术后3年BS_us\n",
    "bs_x_us = df_model_bs_3Y[['staging_after_3Y']]\n",
    "est_us = CoxPHSurvivalAnalysis(ties=\"efron\").fit(bs_x_us, bs_y)\n",
    "survs_us = est_us.predict_survival_function(bs_x_us)\n",
    "preds_us = [fn(10) for fn in survs_us]\n",
    "times, bs_3Y_us = brier_score(bs_y, bs_y, preds_us, 10)\n",
    "print(bs_3Y_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 术后3年BS_ajcc\n",
    "bs_x_ajcc=df_model_bs_3Y[['AJCC_8']]\n",
    "est_ajcc = CoxPHSurvivalAnalysis(ties=\"efron\").fit(bs_x_ajcc, bs_y)\n",
    "survs_ajcc = est_ajcc.predict_survival_function(bs_x_ajcc)\n",
    "preds_ajcc = [fn(10) for fn in survs_ajcc]\n",
    "times, bs_3Y_ajcc = brier_score(bs_y, bs_y, preds_ajcc, 10)\n",
    "print(bs_3Y_ajcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 术后3年BS_gazz\n",
    "bs_x_gazz=df_model_bs_3Y[['Gazzaniga_T']]\n",
    "est_gazz = CoxPHSurvivalAnalysis(ties=\"efron\").fit(bs_x_gazz, bs_y)\n",
    "survs_gazz = est_gazz.predict_survival_function(bs_x_gazz)\n",
    "preds_gazz = [fn(10) for fn in survs_gazz]\n",
    "times, bs_3Y_gazz = brier_score(bs_y, bs_y, preds_gazz, 10)\n",
    "print(bs_3Y_gazz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 术后3年BS_mskcc\n",
    "bs_x_mskcc=df_model_bs_3Y[['MSKCC']]\n",
    "est_mskcc = CoxPHSurvivalAnalysis(ties=\"efron\").fit(bs_x_mskcc, bs_y)\n",
    "survs_mskcc = est_mskcc.predict_survival_function(bs_x_mskcc)\n",
    "preds_mskcc = [fn(10) for fn in survs_mskcc]\n",
    "times, bs_3Y_mskcc = brier_score(bs_y, bs_y, preds_mskcc, 10)\n",
    "print(bs_3Y_mskcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 术后3年BS_blumgart\n",
    "bs_x_blumgart=df_model_bs_3Y[['Blumgart_T']]\n",
    "est_blumgart = CoxPHSurvivalAnalysis(ties=\"efron\").fit(bs_x_blumgart, bs_y)\n",
    "survs_blumgart = est_blumgart.predict_survival_function(bs_x_blumgart)\n",
    "preds_blumgart = [fn(10) for fn in survs_blumgart]\n",
    "times, bs_3Y_blumgart = brier_score(bs_y, bs_y, preds_blumgart, 10)\n",
    "print(bs_3Y_blumgart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 未插补模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,average_precision_score,precision_recall_curve\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score,accuracy_score\n",
    "\n",
    "import xgboost\n",
    "# XGBoost模型\n",
    "xgb_model=xgboost.XGBClassifier(max_depth=5,\n",
    "                        learning_rate=0.018,\n",
    "                        n_estimators=500,\n",
    "                        min_child_weight=0.6,\n",
    "                        eta=0.1,\n",
    "                        gamma=0.5,\n",
    "                        reg_lambda=5,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.6,\n",
    "                        nthread=4,\n",
    "                        scale_pos_weight=1,\n",
    "                        random_state=3)\n",
    "\n",
    "xgb_model.fit(tran_x_sm,tran_y_sm)\n",
    "xgb_predictions=xgb_model.predict(test_x)\n",
    "\n",
    "\n",
    "import lightgbm\n",
    "# LightGBM模型\n",
    "lgbm_model=lightgbm.LGBMClassifier(iterations=300, \n",
    "                                  max_depth=8,\n",
    "                                  min_child_weight=0.9,\n",
    "                                  gamma=0.5,\n",
    "                                   reg_lambda=5,\n",
    "                                  subsample=0.4,\n",
    "                                  learning_rate=0.2, \n",
    "                                  loss_function='CrossEntropy',\n",
    "                                  random_state=3)\n",
    "lgbm_model.fit(tran_x_sm,tran_y_sm)\n",
    "lgbm_predictions=lgbm_model.predict(test_x)\n",
    "\n",
    "\n",
    "import catboost\n",
    "# CatBoost模型\n",
    "cat_model=catboost.CatBoostClassifier(iterations=300, \n",
    "                                      learning_rate=0.2, \n",
    "                                      depth=6,\n",
    "                                      l2_leaf_reg=2,\n",
    "                                      loss_function='MultiClass',\n",
    "                                      random_state=3)\n",
    "cat_model.fit(tran_x_sm,tran_y_sm)\n",
    "cat_predictions=cat_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "# 统一模型输出结果\n",
    "df_model_result=pd.DataFrame(\n",
    "    columns=['model','index','precision','recall','f1-score','support','accuracy','AUC','sensitivity','specificity'])\n",
    "\n",
    "model_list=[xgb_model,lgbm_model,cat_model]\n",
    "model_name_list=['XGBoost','LGBM','CatBoost']\n",
    "for model,name in zip(model_list,model_name_list):\n",
    "#     print(name)\n",
    "    # 计算accuracy和AUC\n",
    "    if name == 'TabNet':\n",
    "        test_x=test_x.to_numpy()\n",
    "    test_y_score=model.predict_proba(test_x)\n",
    "    auc=roc_auc_score(test_y,test_y_score,average='micro',multi_class='ovr')\n",
    "    auc=round(auc,4)\n",
    "    accuracy=accuracy_score(test_y,model.predict(test_x))\n",
    "    accuracy=round(accuracy,4)\n",
    "    # 计算灵敏度sensitivity和特异度specificity\n",
    "    # 计算灵敏度、特异度\n",
    "#     tn, fp, fn, tp = confusion_matrix(test_y,model.predict(test_x),multi_class='ovr').rival()\n",
    "#     sensitivity=round(tp/(tp+fn),4)\n",
    "#     specificity=round(tn/(fp+tn),4)\n",
    "    df_model_result.loc[df_model_result.shape[0],['model','accuracy','AUC']]=\\\n",
    "                                                              [name,accuracy,auc]\n",
    "    # 并入二分类的P-R-f1\n",
    "    # 提取classification_report结果\n",
    "    report = classification_report(test_y, model.predict(test_x), output_dict=True)  # output_dict转化为字典类型\n",
    "    df_report = pd.DataFrame(report).transpose()  # 转置\n",
    "    df_report=df_report.apply(lambda x: round(x,4),axis=0)\n",
    "    df_report=df_report.reset_index(drop=True)\n",
    "    df_model_result=pd.concat([df_model_result,df_report.loc[0:5,:].reset_index()],axis=0)\n",
    "    df_model_result=df_model_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result.rename(columns={'model':'',\n",
    "                               'index':'label'},inplace=True)\n",
    "# 保存模型测试效果\n",
    "df_model_result.to_excel(project_path+'/data/df_分类_模型测试效果_未插补.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 插补模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import r2_score,average_precision_score,precision_recall_curve\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score,accuracy_score\n",
    "\n",
    "import xgboost\n",
    "# XGBoost模型\n",
    "xgb_model=xgboost.XGBClassifier(max_depth=5,\n",
    "                        learning_rate=0.018,\n",
    "                        n_estimators=500,\n",
    "                        min_child_weight=0.6,\n",
    "                        eta=0.1,\n",
    "                        gamma=0.5,\n",
    "                        reg_lambda=5,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.6,\n",
    "                        nthread=4,\n",
    "                        scale_pos_weight=1,\n",
    "                        random_state=3)\n",
    "\n",
    "xgb_model.fit(tran_x_sm,tran_y_sm)\n",
    "xgb_predictions=xgb_model.predict(test_x)\n",
    "\n",
    "\n",
    "import lightgbm\n",
    "# LightGBM模型\n",
    "lgbm_model=lightgbm.LGBMClassifier(iterations=300, \n",
    "                                  max_depth=8,\n",
    "                                  min_child_weight=0.9,\n",
    "                                  gamma=0.5,\n",
    "                                   reg_lambda=5,\n",
    "                                  subsample=0.4,\n",
    "                                  learning_rate=0.2, \n",
    "                                  loss_function='CrossEntropy',\n",
    "                                  random_state=3)\n",
    "lgbm_model.fit(tran_x_sm,tran_y_sm)\n",
    "lgbm_predictions=lgbm_model.predict(test_x)\n",
    "\n",
    "\n",
    "import catboost\n",
    "# CatBoost模型\n",
    "cat_model=catboost.CatBoostClassifier(iterations=300, \n",
    "                                      learning_rate=0.2, \n",
    "                                      depth=6,\n",
    "                                      l2_leaf_reg=2,\n",
    "                                      loss_function='MultiClass',\n",
    "                                      random_state=3)\n",
    "cat_model.fit(tran_x_sm,tran_y_sm)\n",
    "cat_predictions=cat_model.predict(test_x)\n",
    "\n",
    "\n",
    "# 随机森林\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 列出参数列表\n",
    "tree_grid_parameter = {'n_estimators': list((10, 50, 100, 150, 200))}\n",
    "# 进行参数的搜索组合\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid=tree_grid_parameter, cv=3)\n",
    "# 根据已有数据去拟合随机森林模型\n",
    "grid.fit(tran_x_sm, tran_y_sm)\n",
    "rf_model = RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'],\n",
    "                            max_depth=8,\n",
    "                            random_state=3)\n",
    "rf_model.fit(tran_x_sm, tran_y_sm)\n",
    "# 预测缺失值\n",
    "rf_predictions = rf_model.predict(test_x)\n",
    "\n",
    "\n",
    "# GBDT\n",
    "# 列出参数列表\n",
    "gbdt_model = GradientBoostingClassifier(n_estimators=300,\n",
    "                            learning_rate=0.1,\n",
    "                            max_depth=8,\n",
    "                            subsample=0.4,\n",
    "                            random_state=3)\n",
    "gbdt_model.fit(tran_x_sm,tran_y_sm)\n",
    "# 预测缺失值\n",
    "gbdt_predictions = gbdt_model.predict(test_x)\n",
    "\n",
    "\n",
    "# SVR\n",
    "from sklearn.svm import SVR,SVC\n",
    "# 回归模型\n",
    "# svr = SVR(kernel='linear', C=1.25)\n",
    "# 分类模型\n",
    "svr_model = SVC(kernel='rbf',\n",
    "          C=50,\n",
    "          cache_size=200,\n",
    "            probability=True,\n",
    "          random_state=3)\n",
    "svr_model.fit(tran_x_sm,tran_y_sm)\n",
    "svr_predictions=svr_model.predict(test_x)\n",
    "\n",
    "\n",
    "# Linear回归，Lasso回归，领回归，logistic回归\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet,LogisticRegression\n",
    "lcv_model = LogisticRegression(penalty='l2',\n",
    "                         C=5,\n",
    "                        solver='lbfgs',\n",
    "                         max_iter=100,\n",
    "                        random_state=3)\n",
    "# lcv = Lasso()\n",
    "# lcv = Ridge()\n",
    "lcv_model.fit(tran_x_sm, tran_y_sm)\n",
    "lcv_predictions = lcv_model.predict(test_x)\n",
    "\n",
    "# ANN\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "ANN_model = MLPClassifier(alpha=0.1, \n",
    "                    hidden_layer_sizes=[100,], \n",
    "                    solver='adam', \n",
    "                    activation='relu', \n",
    "                    random_state=3)\n",
    "ANN_model.fit(tran_x_sm, tran_y_sm)\n",
    "ANN_predictions=ANN_model.predict(test_x)\n",
    "\n",
    "\n",
    "# TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "TabNet_model = TabNetMultiTaskClassifier(n_d=8, \n",
    "                               n_a=8,\n",
    "                               n_steps=3, # Number of steps in the architecture (usually between 3 and 10)\n",
    "                               gamma=1.5,\n",
    "                               n_independent=2)  #TabNetRegressor()\n",
    "tran_x_x, tran_x_valid, tran_y_y, tran_y_valid = train_test_split(tran_x_sm, tran_y_sm, test_size=0.125, random_state=3)\n",
    "\n",
    "TabNet_model.fit(X_train=tran_x_x.to_numpy(), \n",
    "        y_train=tran_y_y.to_numpy().reshape(-1,1),\n",
    "        max_epochs=200, \n",
    "        patience=20,\n",
    "        batch_size=1024, \n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        loss_fn=[torch.nn.functional.cross_entropy]) # Optional, just an example of list usage\n",
    "\n",
    "TabNet_predictions=TabNet_model.predict(test_x.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统一模型输出结果\n",
    "df_model_result=pd.DataFrame(\n",
    "    columns=['model','index','precision','recall','f1-score','support','accuracy','AUC','sensitivity','specificity'])\n",
    "\n",
    "model_list=[xgb_model,lgbm_model,cat_model,rf_model,gbdt_model,svr_model,lcv_model,ANN_model,TabNet_model]\n",
    "model_name_list=['XGBoost','LGBM','CatBoost','RF','GBDT','SVR','LR','ANN','TabNet']\n",
    "for model,name in zip(model_list,model_name_list):\n",
    "#     print(name)\n",
    "    if name == 'TabNet':\n",
    "        test_x=test_x.to_numpy()\n",
    "    test_y_score=model.predict_proba(test_x)\n",
    "    auc=roc_auc_score(test_y,test_y_score,multi_class='ovr')\n",
    "    auc=round(auc,4)\n",
    "    accuracy=accuracy_score(test_y,model.predict(test_x))\n",
    "    accuracy=round(accuracy,4)\n",
    "    # 计算precision、recall、F1\n",
    "    precision=precision_score(test_y,model.predict(test_x),average='macro')\n",
    "    recall=recall_score(test_y,model.predict(test_x),average='macro')\n",
    "    f1=f1_score(test_y,model.predict(test_x),average='macro')\n",
    "    # 计算accuracy和AUC\n",
    "#     if name == 'TabNet':\n",
    "#         test_x=test_x.to_numpy()\n",
    "#     y_one_hot = label_binarize(test_y, np.arange(6))\n",
    "#     test_y_score=model.predict_proba(test_x)\n",
    "#     auc=roc_auc_score(y_one_hot,test_y_score,average='micro')\n",
    "#     auc=round(auc,4)\n",
    "\n",
    "#     accuracy=accuracy_score(test_y,model.predict(test_x),average='macro')\n",
    "#     accuracy=round(accuracy,2)\n",
    "#     # 计算灵敏度sensitivity和特异度specificity\n",
    "#     # 计算灵敏度、特异度、假阴性率、假阳性率\n",
    "#     tn, fp, fn, tp = confusion_matrix(test_y,model.predict(test_x)).ravel()\n",
    "#     sensitivity=round(tp/(tp+fn),2)\n",
    "#     specificity=round(tn/(fp+tn),2)\n",
    "#     FPR=round(fp/(fp+tn),2)\n",
    "#     FNR=round(fn/(fn+tp),2)\n",
    "#     # 计算约登指数\n",
    "#     youden_index=sensitivity+specificity-1\n",
    "    df_model_result.loc[df_model_result.shape[0],['model','accuracy','AUC',]]=\\\n",
    "                                            [name,accuracy,auc]\n",
    "    # 并入二分类的P-R-f1\n",
    "    # 提取classification_report结果\n",
    "    report = classification_report(test_y, model.predict(test_x), output_dict=True)  # output_dict转化为字典类型\n",
    "    df_report = pd.DataFrame(report).transpose()  # 转置\n",
    "    df_report=df_report.apply(lambda x: round(x,2),axis=0)\n",
    "    df_report=df_report.reset_index(drop=True)\n",
    "    df_model_result=pd.concat([df_model_result,df_report.loc[0:1,:].reset_index()],axis=0)\n",
    "    df_model_result=df_model_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result.rename(columns={'model':'',\n",
    "                               'index':'label'},inplace=True)\n",
    "# 保存模型测试效果\n",
    "df_model_result.to_excel(project_path+'/data/df_分类_模型测试效果_插补.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接使用xgboost和catboost包，而不是auto_ml\n",
    "from sklearn.metrics import r2_score,precision_score,recall_score,f1_score,auc,accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix,average_precision_score,roc_curve,precision_recall_curve\n",
    "\n",
    "import xgboost\n",
    "# XGBoost模型\n",
    "xgb_model=xgboost.XGBClassifier(max_depth=3,\n",
    "                        learning_rate=0.009,\n",
    "                        n_estimators=500,\n",
    "                        min_child_weight=0.4,\n",
    "                        eta=0.1,\n",
    "                        gamma=0.4,\n",
    "                        reg_lambda=10,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        nthread=4,\n",
    "                        scale_pos_weight=1,\n",
    "                        random_state=3)\n",
    "xgb_model.fit(tran_x_sm,tran_y_sm)\n",
    "xgb_predictions=xgb_model.predict(test_x)\n",
    "\n",
    "xgb_precision = precision_score(test_y,xgb_predictions,average='macro')\n",
    "xgb_recall=recall_score(test_y,xgb_predictions,average='macro')\n",
    "xgb_f1=f1_score(test_y,xgb_predictions,average='macro')\n",
    "\n",
    "# test_y_score=model.predict_proba(test_x)[:-1]\n",
    "# xgb_auc=roc_auc_score(test_y,test_y_score,multi_class='ovr')\n",
    "# xgb_auc=round(xgb_auc,2)\n",
    "\n",
    "xgb_accuracy=accuracy_score(test_y,xgb_predictions)\n",
    "print(xgb_precision,xgb_recall,xgb_f1,xgb_auc,xgb_accuracy)\n",
    "\n",
    "# xgb_fpr, xgb_tpr, thresholds = roc_curve(test_y, test_y_score, pos_label=1)  #pos_label=2，表示值为2的实际值为正样本\n",
    "# xgb_auc=auc(xgb_fpr,xgb_tpr)\n",
    "\n",
    "cm2_LogR_model = confusion_matrix(test_y, xgb_model.predict(test_x))\n",
    "print(cm2_LogR_model) #混肴矩阵\n",
    "print(classification_report(test_y, xgb_model.predict(test_x)))\n",
    "\n",
    "# # 计算灵敏度、特异度\n",
    "# xgb_tn, xgb_fp, xgb_fn, xgb_tp = confusion_matrix(test_y,xgb_model.predict(test_x)).ravel()\n",
    "# xgb_sensitivity=round(xgb_tp/(xgb_tp+xgb_fn),3)\n",
    "# xgb_specificity=round(xgb_tn/(xgb_fp+xgb_tn),3)\n",
    "# print(xgb_tn, xgb_fp, xgb_fn, xgb_tp)\n",
    "# print(xgb_sensitivity,xgb_specificity)\n",
    "# 提取classification_report结果\n",
    "# 令output_dict转化为字典类型\n",
    "report = classification_report(test_y, xgb_model.predict(test_x), output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()  # 转置\n",
    "df_report = df_report.reset_index(drop=True)\n",
    "print(df_report)\n",
    "# xgb_ap = average_precision_score(test_y, predictions)\n",
    "# xgb_precision, xgb_recall, _ = precision_recall_curve(test_y, predictions)\n",
    "# # print(predictions)\n",
    "# print(classification_report(test_y, xgb_model.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result=pd.DataFrame(columns=['model','index','precision','recall','f1-score','support','accuracy','AUC','sensitivity','specificity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result.loc[df_model_result.shape[0],['model','accuracy','AUC','sensitivity','specificity']]=\\\n",
    "                                                                    ['xgb',xgb_accuracy,xgb_auc,xgb_sensitivity,xgb_specificity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result=pd.concat([df_model_result,df_report.loc[0:1,:].reset_index()],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result.rename(columns={'model':'',\n",
    "                               'index':''},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?lgbm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "# LightGBM模型\n",
    "lgbm_model=lightgbm.LGBMClassifier(iterations=300, \n",
    "                                  max_depth=8,\n",
    "                                  min_child_weight=0.9,\n",
    "                                  gamma=0.5,\n",
    "                                   reg_lambda=5,\n",
    "                                  subsample=0.4,\n",
    "                                  learning_rate=0.2, \n",
    "                                  loss_function='CrossEntropy',\n",
    "                                  random_state=3)\n",
    "lgbm_model.fit(tran_x_sm,tran_y_sm)\n",
    "lgbm_predictions=lgbm_model.predict(test_x)\n",
    "\n",
    "lgbm_precision = precision_score(test_y,lgbm_predictions)\n",
    "lgbm_recall=recall_score(test_y,lgbm_predictions)\n",
    "lgbm_f1=f1_score(test_y,lgbm_predictions)\n",
    "\n",
    "test_y_score=lgbm_model.predict_proba(test_x)[:,-1]\n",
    "lgbm_auc=roc_auc_score(test_y,test_y_score)\n",
    "lgbm_accuracy=accuracy_score(test_y,lgbm_predictions)\n",
    "print('lgbm',lgbm_precision,lgbm_recall,lgbm_f1,lgbm_auc,lgbm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?catboost.CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "# CatBoost模型\n",
    "cat_model=catboost.CatBoostClassifier(iterations=300, \n",
    "                                      learning_rate=0.2, \n",
    "                                      depth=6,\n",
    "                                      l2_leaf_reg=2,\n",
    "                                      subsample=1,\n",
    "                                      loss_function='CrossEntropy',\n",
    "                                      random_state=3)\n",
    "\n",
    "cat_model.fit(tran_x_sm,tran_y_sm)\n",
    "cat_predictions=cat_model.predict(test_x)\n",
    "\n",
    "cat_precision = precision_score(test_y,cat_predictions)\n",
    "cat_recall=recall_score(test_y,cat_predictions)\n",
    "cat_f1=f1_score(test_y,cat_predictions)\n",
    "\n",
    "test_y_score=cat_model.predict_proba(test_x)[:,-1]\n",
    "cat_auc=roc_auc_score(test_y,test_y_score)\n",
    "cat_accuracy=accuracy_score(test_y,cat_predictions)\n",
    "print('catboost',cat_precision,cat_recall,cat_f1,cat_auc,cat_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('catboost',cat_precision,cat_recall,cat_f1,cat_auc,cat_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林，GBDT\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 列出参数列表\n",
    "tree_grid_parameter = {'n_estimators': list((10, 50, 100, 150, 200))}\n",
    "# 进行参数的搜索组合\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid=tree_grid_parameter, cv=3)\n",
    "# 根据已有数据去拟合随机森林模型\n",
    "grid.fit(tran_x_sm, tran_y_sm)\n",
    "rf_model = RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'],\n",
    "                            max_depth=8,\n",
    "                            random_state=3)\n",
    "rf_model.fit(tran_x_sm, tran_y_sm)\n",
    "\n",
    "\n",
    "# 预测缺失值\n",
    "rf_predictions = rf_model.predict(test_x)\n",
    "\n",
    "rf_precision = precision_score(test_y,rf_predictions)\n",
    "rf_recall=recall_score(test_y,rf_predictions)\n",
    "rf_f1=f1_score(test_y,rf_predictions)\n",
    "\n",
    "test_y_score=rf_model.predict_proba(test_x)[:,-1]\n",
    "rf_auc=roc_auc_score(test_y,test_y_score)\n",
    "rf_accuracy=accuracy_score(test_y,rf_predictions)\n",
    "print('rf',rf_precision,rf_recall,rf_f1,rf_auc,rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 列出参数列表\n",
    "tree_grid_parameter = {'n_estimators': [200,300,500],\n",
    "                      'learning_rate': [0.01,0.1],\n",
    "                      'max_depth': [3,5],\n",
    "                      'subsample':[0.5,0.6,0.8]}\n",
    "# 进行参数的搜索组合\n",
    "grid = GridSearchCV(GradientBoostingClassifier(), param_grid=tree_grid_parameter, cv=3)\n",
    "# 根据已有数据去拟合随机森林模型\n",
    "grid.fit(tran_x_sm, tran_y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_model = GradientBoostingClassifier(n_estimators=300,\n",
    "                            learning_rate=0.1,\n",
    "                            max_depth=8,\n",
    "                            subsample=0.4,\n",
    "                            random_state=3)\n",
    "gbdt_model.fit(tran_x_sm, tran_y_sm)\n",
    "# 预测缺失值\n",
    "gbdt_predictions = gbdt_model.predict(test_x)\n",
    "\n",
    "gbdt_precision = precision_score(test_y,gbdt_predictions)\n",
    "gbdt_recall=recall_score(test_y,gbdt_predictions)\n",
    "gbdt_f1=f1_score(test_y,gbdt_predictions)\n",
    "\n",
    "test_y_score=gbdt_model.predict_proba(test_x)[:,-1]\n",
    "gbdt_auc=roc_auc_score(test_y,test_y_score)\n",
    "gbdt_accuracy=accuracy_score(test_y,gbdt_predictions)\n",
    "print('gbdt',gbdt_precision,gbdt_recall,gbdt_f1,gbdt_auc,gbdt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "from sklearn.svm import SVR,SVC\n",
    "# 回归模型\n",
    "# svr = SVR(kernel='linear', C=1.25)\n",
    "# 分类模型\n",
    "svr_model = SVC(kernel='rbf',\n",
    "          C=50,\n",
    "          cache_size=200,\n",
    "            probability=True,\n",
    "          random_state=3)\n",
    "svr_model.fit(tran_x_sm,tran_y_sm)\n",
    "svr_predictions=svr_model.predict(test_x)\n",
    "\n",
    "svr_precision = precision_score(test_y,svr_predictions)\n",
    "svr_recall=recall_score(test_y,svr_predictions)\n",
    "svr_f1=f1_score(test_y,svr_predictions)\n",
    "\n",
    "test_y_score=svr_model.predict_proba(test_x)[:,-1]\n",
    "svr_auc=roc_auc_score(test_y,test_y_score)\n",
    "svr_accuracy=accuracy_score(test_y,svr_predictions)\n",
    "print('SVC',svr_precision,svr_recall,svr_f1,svr_auc,svr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?svr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN训练\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(tran_x,tran_y)\n",
    "predictions=knn.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear回归，Lasso回归，领回归\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet,LogisticRegression\n",
    "lcv_model = LogisticRegression(penalty='l2',\n",
    "                         C=5,\n",
    "                        solver='lbfgs',\n",
    "                         max_iter=100,\n",
    "                        random_state=3)\n",
    "# lcv = Lasso()\n",
    "# lcv = Ridge()\n",
    "\n",
    "lcv_model.fit(tran_x_sm, tran_y_sm)\n",
    "lcv_predictions = lcv_model.predict(test_x)\n",
    "\n",
    "lcv_precision = precision_score(test_y,lcv_predictions)\n",
    "lcv_recall=recall_score(test_y,lcv_predictions)\n",
    "lcv_f1=f1_score(test_y,lcv_predictions)\n",
    "\n",
    "test_y_score=lcv_model.predict_proba(test_x)[:,-1]\n",
    "lcv_auc=roc_auc_score(test_y,test_y_score)\n",
    "lcv_accuracy=accuracy_score(test_y,lcv_predictions)\n",
    "print('lr',lcv_precision,lcv_recall,lcv_f1,lcv_auc,lcv_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "ann_params = {'alpha': [0.0001,0.001,0.01,0.1,0.8],\n",
    "              \"solver\": ['adam', 'sgd', 'lbfgs'],\n",
    "              'hidden_layer_sizes': [(100,), (50,), (20,), (10,)]}\n",
    "\n",
    "ANN = MLPClassifier(random_state=3)\n",
    "grid = GridSearchCV(ANN, \n",
    "                    ann_params, \n",
    "                    cv=3, \n",
    "                    scoring='f1'\n",
    "                   )\n",
    "grid.fit(tran_x_sm, tran_y_sm)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "ANN = MLPClassifier(alpha=0.1, \n",
    "                    hidden_layer_sizes=[100,], \n",
    "                    solver='adam', \n",
    "                    activation='relu', \n",
    "                    random_state=3)\n",
    "ANN.fit(tran_x_sm, tran_y_sm)\n",
    "ann_predictions=ANN.predict(test_x)\n",
    "\n",
    "ann_precision = precision_score(test_y,ann_predictions)\n",
    "ann_recall=recall_score(test_y,ann_predictions)\n",
    "ann_f1=f1_score(test_y,ann_predictions)\n",
    "\n",
    "test_y_score=ANN.predict_proba(test_x)[:,-1]\n",
    "ann_auc=roc_auc_score(test_y,test_y_score)\n",
    "ann_accuracy=accuracy_score(test_y,ann_predictions)\n",
    "print('ann',ann_precision,ann_recall,ann_f1,ann_auc,ann_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(test_y, predictions))\n",
    "\n",
    "cm2_LogR_model = confusion_matrix(test_y, predictions)\n",
    "print(cm2_LogR_model) #混肴矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "os.chdir('D:\\Anaconda3\\Lib\\site-packages\\~-rch\\lib')\n",
    "ctypes.cdll.LoadLibrary('caffe2_nvrtc.dll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNet\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "TabNet_model = TabNetMultiTaskClassifier(\n",
    "                       cat_emb_dim=1,\n",
    "                       optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=2e-2),\n",
    "                       scheduler_params={\"step_size\":50, # how to use learning rate scheduler\n",
    "                                         \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='entmax') # \"sparsemax\"\n",
    "tran_x_x, tran_x_valid, tran_y_y, tran_y_valid = train_test_split(tran_x_sm, tran_y_sm, test_size=0.125, random_state=3)\n",
    "\n",
    "TabNet_model.fit(X_train=tran_x_x, \n",
    "        y_train=tran_y_y.reshape(-1,1),\n",
    "        max_epochs=200, \n",
    "        patience=20,\n",
    "        batch_size=1024, \n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        loss_fn=[torch.nn.functional.cross_entropy]) # Optional, just an example of list usage\n",
    "\n",
    "tabnet_predictions=TabNet_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_score=model.predict_proba(test_x)\n",
    "auc=roc_auc_score(test_y,test_y_score,multi_class='ovr')\n",
    "auc=round(auc,4)\n",
    "accuracy=accuracy_score(test_y,model.predict(test_x))\n",
    "accuracy=round(accuracy,4)\n",
    "# 计算precision、recall、F1\n",
    "precision=precision_score(test_y,model.predict(test_x),average='macro')\n",
    "recall=recall_score(test_y,model.predict(test_x),average='macro')\n",
    "f1=f1_score(test_y,model.predict(test_x),average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "TabNet_model = TabNetClassifier(n_d=8, \n",
    "                               n_a=8,\n",
    "                               n_steps=3, # Number of steps in the architecture (usually between 3 and 10)\n",
    "                               gamma=1.5,\n",
    "                               n_independent=2)  #TabNetRegressor()\n",
    "tran_x_x, tran_x_valid, tran_y_y, tran_y_valid = train_test_split(tran_x_sm, tran_y_sm, test_size=0.125, random_state=3)\n",
    "\n",
    "TabNet_model.fit(X_train=tran_x_x.to_numpy(), \n",
    "        y_train=tran_y_y.to_numpy(), \n",
    "        eval_set=[(tran_x_valid.to_numpy(), tran_y_valid.to_numpy())], \n",
    "        eval_name=['train'], \n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=100,\n",
    "        patience=50,\n",
    "        batch_size=128,\n",
    "        virtual_batch_size=14,\n",
    "        num_workers=0,\n",
    "        drop_last=False)\n",
    "\n",
    "tab_predictions=TabNet_model.predict(test_x.to_numpy())\n",
    "\n",
    "tab_precision = precision_score(test_y,tab_predictions)\n",
    "tab_recall=recall_score(test_y,tab_predictions)\n",
    "tab_f1=f1_score(test_y,tab_predictions)\n",
    "\n",
    "test_y_score = TabNet_model.predict_proba(test_x.to_numpy())[:,-1]\n",
    "tab_auc = roc_auc_score(y_score=test_y_score, y_true=test_y)\n",
    "tab_accuracy=accuracy_score(test_y,tab_predictions)\n",
    "print('tab',tab_precision,tab_recall,tab_f1,tab_auc,tab_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tab',tab_precision,tab_recall,tab_f1,tab_auc,tab_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "predictions=TabNet_model.predict(test_x.to_numpy())\n",
    "print(classification_report(test_y, predictions))\n",
    "\n",
    "cm2_LogR_model = confusion_matrix(test_y, predictions)\n",
    "print(cm2_LogR_model) #混肴矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算R2和均方误差MSE\n",
    "print('-----------------------计算R2和均方误差MSE---------------------------')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error  # 均方误差\n",
    "from sklearn.metrics import mean_absolute_error  # 平方绝对误差\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score  # R square\n",
    "# 调用\n",
    "\n",
    "r2 = precision_score(test_y,xgb_predictions)\n",
    "print('precision: ',r2)\n",
    "mse=recall_score(test_y,xgb_predictions)\n",
    "print('recall: ',mse)\n",
    "mae=f1_score(test_y,xgb_predictions)\n",
    "print('f1: ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions= pd.DataFrame(data={'真实值':test_y,'预测值':xgb_predictions})\n",
    "writer = pd.ExcelWriter(project_path + '/data/df_model_测试集结果.xlsx')\n",
    "df_predictions.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要性评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance =pd.read_excel(project_path +'/data/result/modeling/df_1.6_模型重要性评分_after.xlsx')\n",
    "if 'Unnamed: 0' in df_importance.columns:\n",
    "    df_importance = df_importance.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance['重要性评分']=df_importance['重要性评分'].apply(lambda x: round(x,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance.特征.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance['特征']=['手术结果','AJCC_8分期','血型','肿瘤指标CA19-9','MSKCC分期','肿瘤指标CA125','乙肝核心抗体','输血浆','乙肝表面抗原',\n",
    "                     '术中出血','肿瘤最大径','吸烟史','有无消瘦','饮酒史','直接胆红素','肿瘤指标AFP','肿瘤指标CEA','术前减黄','有无胆道系统疾病',\n",
    "                     '性别','有无肝硬化']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']  ##绘图显示中文\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rc\n",
    "\n",
    "names = df_importance['特征']\n",
    "index = np.arange(len(names))\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(df_importance['特征'], df_importance['重要性评分'], width=0.7,\n",
    "        color=(0.42941176470588235, 0.7294117647058823, 0.7490196078431373), tick_label=names)\n",
    "# 设置坐标刻度值的大小\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "plt.ylabel('Importance Score',fontsize=20)\n",
    "plt.xlabel('Variable Name',fontsize=20)\n",
    "for a, b in zip(index, df_importance['重要性评分']):\n",
    "    plt.text(a, b + 0.002, '%.4f' % b, ha='center', va='bottom', fontsize=10)\n",
    "# plt.title('重要变量得分柱形图')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 散点图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断文件路径是否存在，如果不存在则创建该路径\n",
    "def mkdir(path):\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:  # 判断是否存在文件夹如果不存在则创建为文件夹\n",
    "        os.makedirs(path)  # makedirs 创建文件时如果路径不存在会创建这个路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "print('-----------------------画图---------------------------')\n",
    "from pylab import mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']  ##绘图显示中文\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')\n",
    "\n",
    "# 散点图\n",
    "# axis设置坐标轴的范围\n",
    "# plt.axis([-20, 20, 0, 200])\n",
    "# x为x轴中坐标x的值，y为y轴中坐标y的值，x与y都是长度相同的数组序列，color为点的颜色，marker为散点的形状，\n",
    "# 折线图刻度调小，要不然点都堆到一块了\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(0,10)\n",
    "ax.set_ylim(0,10)\n",
    "# plt.scatter(range(len(test_y)),test_y,c='r')\n",
    "plt.scatter(test_y,predictions,c='b')\n",
    "# 红色参照线\n",
    "plt.plot(list(range(test_y.shape[0])), list(range(test_y.shape[0])),color='r')\n",
    "# plt.plot(list(range(30)), list(range(30)),color='r')\n",
    "plt.xlabel('Number of Events(unit)')\n",
    "plt.ylabel('MTX Bone Suppression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show()\n",
    "# 判断图片保存路径是否存在，否则创建\n",
    "jpg_path = project_path + \"/jpg\"\n",
    "mkdir(jpg_path)\n",
    "plt.savefig(jpg_path + \"/他克莫司血药浓度测试集散点图v2.0.jpg\", dpi=300)\n",
    "plt.clf()  # 删除前面所画的图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']  ##绘图显示中文\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# plt.plot(logistic_fpr, logistic_tpr,label='LogisticRegression(AUC = %0.2f)' % logistic_auc) \n",
    "# plt.plot(rf_fpr, rf_tpr,label='RandomForest(AUC = %0.2f)' % rf_auc) \n",
    "plt.plot(xgb_fpr, xgb_tpr,label='XGBoost(AUC = %0.2f)' % xgb_auc) \n",
    "# plt.plot(ann_fpr, ann_tpr,label='ANN(AUC = %0.2f)' % ann_auc) \n",
    "\n",
    "plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC Curve of MTX drug')\n",
    "plt.legend(loc=\"lower right\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(logistic_recall, logistic_precision, label='LogisticRegression(AP = %0.2f)' % logistic_ap)\n",
    "plt.plot(rf_recall, rf_precision,label='RandomForest(AP = %0.2f)' % rf_ap) \n",
    "plt.plot(xgb_recall, xgb_precision,label='XGBoost(AP = %0.2f)' % xgb_ap) \n",
    "plt.plot(ann_recall, ann_precision,label='ANN(AP = %0.2f)' % ann_ap) \n",
    "         \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 0.6])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.legend(loc=\"upper right\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summary_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(tran_y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP图\n",
    "from pylab import mpl\n",
    "from matplotlib import pyplot as plt\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']  ##绘图显示中文\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')\n",
    "\n",
    "import catboost,xgboost\n",
    "import shap\n",
    "shap.initjs()  # notebook环境下，加载用于可视化的JS代码\n",
    "# CatBoost模型\n",
    "cat_model=xgboost.XGBClassifier(max_depth=5,\n",
    "                        learning_rate=0.001,\n",
    "                        n_estimators=500,\n",
    "                        min_child_weight=0.5,\n",
    "                        eta=0.1,\n",
    "                        gamma=0.5,\n",
    "                        reg_lambda=5,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        nthread=4,\n",
    "                        scale_pos_weight=1,\n",
    "                        random_state=3)\n",
    "cat_model.fit(tran_x_sm, tran_y_sm)\n",
    "\n",
    "explainer = shap.TreeExplainer(cat_model)\n",
    "shap_values = explainer.shap_values(tran_x_sm)  # 传入特征矩阵X，计算SHAP值\n",
    "# print(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_x_sm=tran_x_sm.rename(columns={'日剂量':'上一次日剂量',\n",
    "                                   'gender':'性别',\n",
    "                                   'age':'年龄',\n",
    "                                   'test_result':'上一次TDM值'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?shap.summary_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, tran_x_sm,plot_size=(12,8),\n",
    "                 class_names=['1mg','1.5mg','4mg','3mg','2.5mg','2mg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_x_sm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap = pd.DataFrame(data={'features':shap_col,\n",
    "                            'shap_pos':shap_pos_list,\n",
    "                            'shap_neg':shap_neg_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap.to_excel(project_path+'/data/result/df_shap.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_list=[]\n",
    "for i in range(df_shap_values.shape[1]-1):\n",
    "    shap_value=df_shap_values.iloc[:,i].sum()\n",
    "    shap_list.append(shap_value)\n",
    "df_shap = pd.DataFrame(data={'features':tran_x_sm.columns,\n",
    "                            'shap_value':shap_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap_values.iloc[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(project_path + '/data/result/df_shap值排序.xlsx')\n",
    "df_shap.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multioutput_decision_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?shap.multioutput_decision_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=df_model.columns.to_list()\n",
    "col.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.multioutput_decision_plot(shap_values,shap_values,row_index=3,\n",
    "                              feature_names=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混淆矩阵图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import catboost\n",
    "# CatBoost模型\n",
    "cat_model=xgboost.XGBClassifier(max_depth=5,\n",
    "                        learning_rate=0.001,\n",
    "                        n_estimators=500,\n",
    "                        min_child_weight=0.5,\n",
    "                        eta=0.1,\n",
    "                        gamma=0.5,\n",
    "                        reg_lambda=5,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        nthread=4,\n",
    "                        scale_pos_weight=1,\n",
    "                        random_state=3)\n",
    "\n",
    "cat_model.fit(tran_x_sm,tran_y_sm)\n",
    "cat_predictions=cat_model.predict(test_x)\n",
    "# 计算混淆矩阵\n",
    "cat_confusion=confusion_matrix(test_y,cat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cat_confusion, cmap=plt.cm.Blues) # 在特定的窗口上显示图像\n",
    "# 设置图表标题\n",
    "plt.title('XGBoost Confusion Matrix',size=20)    # 图像标题\n",
    "plt.colorbar()\n",
    "# 设置坐标轴标题\n",
    "font_x = {'family': 'Times New Roman',\n",
    "         'weight': 'normal',\n",
    "         'size': 20,}\n",
    "plt.xlabel('prediction label',font_x)\n",
    "plt.ylabel('True label',font_x)\n",
    "# 设置坐标轴刻度\n",
    "plt.tick_params(labelsize=23)  # 设置刻度值大小\n",
    "label_names=['1g','1.5g','2mg','2.5mg','3mg','4mg']\n",
    "plt.xticks(range(len(label_names)),label_names)\n",
    "plt.yticks(range(len(label_names)),label_names)\n",
    "# 显示数据\n",
    "for first_index in range(len(cat_confusion)):    #第几行\n",
    "    for second_index in range(len(cat_confusion[first_index])):    #第几列\n",
    "        print(first_index, second_index)\n",
    "        plt.text(second_index,first_index, cat_confusion[first_index][second_index],size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for first_index in range(len(cat_confusion)):    #第几行\n",
    "    print(cat_confusion[first_index])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for second_index in range(len(cat_confusion[first_index])):    #第几列\n",
    "    print(cat_confusion[first_index][second_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示数据\n",
    "for first_index in range(len(cat_confusion)):    #第几行\n",
    "    for second_index in range(len(cat_confusion[first_index])):    #第几列\n",
    "        print(first_index, second_index)\n",
    "        print(cat_confusion[first_index][second_index])\n",
    "        plt.text(first_index, second_index, cat_confusion[first_index][second_index],size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tabnet mask graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "TabNet_model = TabNetClassifier()  #TabNetRegressor()\n",
    "tran_x_x, tran_x_valid, tran_y_y, tran_y_valid = train_test_split(tran_x_sm, tran_y_sm, test_size=0.1, random_state=3)\n",
    "\n",
    "TabNet_model.fit(X_train=tran_x_x.to_numpy(),  \n",
    "        y_train=tran_y_y.to_numpy(), \n",
    "        eval_set=[(tran_x_valid.to_numpy(), tran_y_valid.to_numpy())], \n",
    "        eval_name=['train'], \n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        batch_size=128,\n",
    "        virtual_batch_size=15,\n",
    "        num_workers=0,\n",
    "        drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix,masks=TabNet_model.explain(tran_x_sm.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# fig = plt.figure(figsize=(40,40))\n",
    "# ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "# plt.yticks(np.arange(0, len(explain_matrix), 1.0))\n",
    "# plt.xticks(np.arange(0, len(explain_matrix[0]), 1.0))\n",
    "# ax.set_xticklabels(tran_x_sm.columns, rotation=75)\n",
    "# plt.ylabel('Sample Number')\n",
    "# plt.xlabel('Variable')\n",
    "# # plt.imshow(explain_matrix[:30])  # 显示30个\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "for i in range(3):\n",
    "    axs[i].set_yticks(np.arange(0, len(explain_matrix), 1.0))  # 设置左边间距\n",
    "    axs[i].set_xticks(np.arange(0, len(explain_matrix[0]), 2.0))\n",
    "    axs[i].set_ylabel('Sample Number',size=20)\n",
    "    axs[i].set_xlabel('Variable',size=20)\n",
    "    # 设置坐标刻度值的大小\n",
    "    axs[i].tick_params(labelsize=15)\n",
    "    axs[i].imshow(masks[i][:30])\n",
    "    axs[i].set_title(f\"mask {i}\")\n",
    "    axs[i].set_xticklabels(tran_x_sm.columns[::2], rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_x_sm.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "460.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
